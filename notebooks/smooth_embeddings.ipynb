{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need cuml for this--you can of course cluster embeddings with `sklearn` or other libraries, but for this project we used cuml because of its relatively fast speed.\n",
    "\n",
    "See: [https://docs.rapids.ai/install#selector](https://docs.rapids.ai/install#selector) for instructions on how to install cuml.\n",
    "\n",
    "For the 24.6 version of the NVIDIA RAPIDS suite (which cuml is a component of), here is the install from the selector:\n",
    "\n",
    "```\n",
    "pip install \\\n",
    "    --extra-index-url=https://pypi.nvidia.com \\\n",
    "    cuml-cu12==24.6.*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook assumes you have generated embeddings from a trained model; here we'll use embeddings from the model we trained on the ABC AIBS MERFISH data. To download this data see the script in `scripts/training/download_aibs.sh` or visit the ABC Atlas website [https://portal.brain-map.org/atlases-and-data/bkp/abc-atlas](https://portal.brain-map.org/atlases-and-data/bkp/abc-atlas).\n",
    "\n",
    "This is minimal code to smooth embeddings generated from a trained model along the spatial cell graph. We basically smooth cells using a weighted average of their neighbors. The weighting function is a simple gaussian kernel with a fixed spatial FWHM/sigma. \n",
    "\n",
    "This can be very slow, so in order to improve the speed of the workflow we use cuml's approximate nearest neighbor code to look at the nearest $n$ neighbors and only smooth over those cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cuml\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\n",
    "    \"/home/ajl/work/d2/code/mousebrain/clustering/embeddings/temp-for-2mmc/2mmc_epoch39_all-cat-sm40_rad17.pth\"\n",
    ")\n",
    "\n",
    "# x = np.load('embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/home/ajl/work/d1/abc/metadata/MERFISH-C57BL6J-638850-CCF/20231215/cell_metadata_with_parcellation_annotation.csv\"\n",
    ")\n",
    "adata = ad.read_h5ad('/home/ajl/work/d1/abc/expression_matrices/MERFISH-C57BL6J-638850/20230630/C57BL6J-638850-log2.h5ad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.cell_label.isin(adata.obs.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True) # we need a contiguous index for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(d, sigma):\n",
    "    return torch.exp(-d**2 / (2 * sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x_reconstructed_scaled'] = np.round(df['x_reconstructed'] * 100)\n",
    "df['y_reconstructed_scaled'] = np.round(df['y_reconstructed'] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.005612043932249\n"
     ]
    }
   ],
   "source": [
    "p = 2 # at the scale we are using e ach \"voxel\" is a non-isotropic 10micron \n",
    "# \"xy\" resolution box \n",
    "sigma = np.sqrt(2 * p**2)*0.5 / (2*np.sqrt(2*np.log(2)))\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:41<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 1000\n",
    "all_embeds = []\n",
    "for uniq_section in tqdm.tqdm(df.brain_section_label.unique()):\n",
    "\tsmoothed_embeds = []\n",
    "\tsection = df[df.brain_section_label==uniq_section]\n",
    "\tpositions = section[['x_reconstructed_scaled', 'y_reconstructed_scaled']].values\n",
    "\n",
    "\tnearest = cuml.NearestNeighbors(n_neighbors=n_neighbors)\n",
    "\tnearest.fit(positions) \n",
    "\tdistances, indices = nearest.kneighbors(positions)\n",
    "\n",
    "\tfor idx, dist in zip(indices, distances):\n",
    "\t\tdist = torch.from_numpy(dist)\n",
    "\t\tweights = gaussian(dist, sigma)\n",
    "\t\tnormalized_wts = weights / weights.sum()\n",
    "\n",
    "\t\tindices_X = section.index[idx]\n",
    "\t\tembeds_neighbors = x[indices_X]\n",
    "\t\tweighted = embeds_neighbors * normalized_wts[:, None]\n",
    "\t\tembeds_weighted = weighted.sum(axis=0)\n",
    "\t\tsmoothed_embeds.append(embeds_weighted)\n",
    "\t\t\n",
    "\tsmoothed_embeds_cat = torch.stack(smoothed_embeds)\n",
    "\t# highly advised to save this and not keep all of them in memory\n",
    "\t#torch.save(smoothed_embeds_cat, f'{uniq_section}_smoothed.pth')\n",
    "\tall_embeds.append(smoothed_embeds_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
