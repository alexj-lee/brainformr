{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>This is documentation for code written as part of the manuscript \"Data-driven fine-grained region discovery in the mouse brain with transformers\". </p>"},{"location":"#installation","title":"Installation","text":"<ul> <li><code>pip install git+github.com:alexj-lee/brainformr.git</code> or clone and pip install; alternatively use the Dockerfile -- <code>PyTorch</code> and some other heavy libraries are required, which could take a couple minutes (&lt;10). It's also a somewhat unoptimized Docker image (no multi-stage build etc.) so caveat emptor.</li> </ul>"},{"location":"#getting-started-with-training-on-different-datasets","title":"Getting started with training on different datasets","text":"<p>Requirements are a CSV file with cell types and cell IDs corresponding to an <code>anndata</code> object with probe counts. To set these up for use with the code in this repo:</p> <ol> <li>make sure you provide the right cardinality (number of cell types) to the model using the <code>model</code> config. You can also do this usinmg the <code>hydra</code> CLI by just passing <code>python [SCRIPT.PY] +model.cell_cardinality=9000</code> (or whatever the value is)</li> <li>the code right now unfortunately assumes that the values in your anndata.X are <code>log1p</code> transformed and then uses <code>.exp()</code> in the implementation (<code>training/lightning_model.py</code>) to produce counts again for <code>scvi.NegativeBinomial</code> -- make sure you follow this convention or edit the training file for a different convention.</li> </ol>"},{"location":"#i-want-to-edit-the-anndata-merfish-probe-counts-and-csv-cell-metadata-to-work-with-this-codebase","title":"I want to edit the anndata (MERFISH probe counts) and CSV (cell metadata) to work with this codebase","text":"<ol> <li> <p>Provide in your CSV (case sensitive):</p> column name description <code>cell_type</code> integer encoded class label for the cell type of a given cell <code>cell_label</code> value that will be used to index the <code>anndata</code> object. Make sure it is of appropriate datatype because we do not perform any transformation on it (such as conversion to str or int) prior to indexing the <code>anndata</code> object. <code>brain_section_label</code> value that we will <code>.groupby()</code> on to select individual tissue sections to get the cells <code>x</code> spatial coordinate that will be used to identify neighbors. Must be in same units as <code>patch_size</code> argument (default in <code>hydra</code> configs is micron). <code>y</code> similar as <code>x</code> 2. Change paths in the <code>hydra</code> config file template (<code>scripts/config/data/mouse1.yaml</code>) 3. Make sure value of <code>patch_size</code> fits your <code>x</code> and <code>y</code> created in step (1) in the same YAML file. 4. Adjust model parameters in <code>scripts/config/model/</code> for desired model. 5. Set <code>config_path</code> in <code>hydra.main</code> decorator to the path and config file of interest (see template file <code>scripts/training/train_base.py</code>). 6. Set up <code>wandb</code> parameters in <code>scripts/training/train_base.py</code>. 7. Run with <code>python train_base.py</code> </li> </ol>"},{"location":"#i-want-to-edit-this-at-the-hydra-config-level-or-in-the-starter-script","title":"I want to edit this at the hydra config level or in the starter script","text":"<ol> <li>Hydra config file setup: change data paths in <code>config/data</code> or create new data yaml file in that folder that has the same fields as the examples in that directory. Make sure to specify:<ul> <li><code>celltype_colname</code>: the column that gives the cell type of the cells in the dataset (if you are templating from the <code>train_aibs_mouse.py</code> file, which we recommend, we will use <code>sklearn.preprocessing.LabelEncoder</code> on this column in the base <code>train_aibs_mouse.py</code> file, so it doesn't matter if it's integer or string encoded. If you are not using that function, make sure the dataframe you pass to <code>CenterMaskSampler</code> has column <code>cell_type</code> (case sensitive default argument) which must integer encoded.)<ul> <li>NOTE: we are basically assuming you want to train on one mouse, so if there are multiple and there is a chance that one mouse out of multiple has some cell types that are not shared, you need to separately fit the LabelEncoder and then provide integer-encoded class labels (see <code>train_zhuang.py</code> for an example.)</li> </ul> </li> <li><code>cell_id_colname</code>: the column that gives an ID that we can use to lookup into the h5ad file for single cells' probe count profiles. Make sure this is of the same datatype as the row ID's in the <code>anndata</code> object (i.e. make sure that the ID isn't 12345: uint | int instead of 12345: str). </li> </ul> </li> <li>specify model architecture (depth, width etc.) configs in <code>config/model</code> yaml file</li> <li>implement the <code>load_data</code> method for <code>BaseTrainer</code> in <code>scripts/training/lightning_model.py</code> (see <code>train_zhuang.py</code> and <code>train_aibs_mouse.py</code> for reference)<ul> <li>in essence this is normalization: mapping spatial x and y coordinates in your data to \"x\" and \"y\" and scaling them, also normalizing cell type column names as described in (1), optionally. One example might be to filter control probes. <ul> <li>specifically you can look at <code>load_data</code> in <code>train_aibs_mouse</code> to see an example, but the dataloader code assumes that the spatial columns are <code>x</code> and <code>y</code>. </li> <li>we also don't automatically rescale the units of the <code>x</code> and <code>y</code> columns relative to the <code>patch_size</code> arguments. The idea is for the user to correctly scaled versions and to use code in <code>scripts/training/lightning_model.py:BaseTrainer.load_data</code> to set up the data loader with the logic you need for your data, and then pass a version of that to <code>brainformr.data.CenterMaskSampler</code></li> </ul> </li> <li>for more information on the data and dataloader, see the data + dataloader page</li> </ul> </li> <li>alternatively just change the <code>data.patch_size</code> config value in <code>hydra</code> (see <code>scripts/config/data/</code>); as long as the desired patch size and spatial units in the dataframe are correctly scaled, then it will work</li> <li>add <code>wandb</code> project if desired to top level config in <code>config</code></li> <li>copy boilerplate for initiating training from <code>train_aibs_mouse.py</code> or <code>train_zhuang.py</code> (ie code in <code>main</code> that); make sure to specify correct config file in the <code>@hydra.main</code> decorator<ul> <li>for more information on this see the <code>hydra</code> docs.</li> </ul> </li> </ol>"},{"location":"#core-code-components-and-usage","title":"Core code components and usage","text":""},{"location":"#config-management-with-hydra","title":"Config management with hydra","text":"<p>The main interface to the training code we wrote is through <code>hydra</code> (https://hydra.cc/), which is a configuration framework that uses yaml files to orchestrate and organize complex workflows. Please see the <code>hydra</code> documentation for more information.</p> <p>The pipeline controls the basic training operations through these yaml files and Pytorch Lightning.</p> <p>For example, <code>scripts/config/model/base.yaml</code> controls the parameters of the transformer itself, for example:</p> <pre><code>_target_: brainformr.model.CellTransformer\nencoder_embedding_dim: 384\ndecoder_embedding_dim: 384\n\nencoder_num_heads: 8\ndecoder_num_heads: 8\nattn_pool_heads: 8\n\nencoder_depth: 4\ndecoder_depth: 4\n\ncell_cardinality: 384\neps: 1e-9\n\nn_genes: 500\nxformer_dropout: 0.0\nbias: True\nzero_attn: True\n</code></pre> <p>We can use hydra to directly instantiate this model (which we specify using the <code>_target_</code> attribute) by specifiying the object class, here <code>brainformr.model.CellTransformer</code>. What this looks like in context is in following snippet:</p> <pre><code>cfg_path = 'config.yaml'\ncfg = OmegaConf.load(cfg_path) # same as above snippet\nmodel = hydra.utils.instantiate(cfg.model)\n\n# model will have 500 gene output decoder depth of 4, etc. and will be an instance of class `brainformr.model.CellTransformer`\n</code></pre>"},{"location":"#composition-of-config-files-is-controlled-at-top-level-using-another-config","title":"Composition of config files is controlled at top-level using another config","text":"<p>An example of this composition at high level is the <code>scripts/config/example.yaml</code> file, which contains the settings used to train on the Allen Institute for Brain Science MERFISH data in the Allen Brain Cell Atlas. Note that \"mouse1.yaml\" refers to a file inside the <code>config/data</code> directory. Correspondingly there is a <code>config/model/base.yaml</code> file that is specified by the below config, which is found one-level-up (ie in the <code>config</code> directory). </p> <pre><code>defaults:\n  - _self_\n  - data: mouse1.yaml\n  - model: base.yaml\n  - optimization: base.yaml\n\ncheckpoint_dir: \nwandb_project: \nmodel_checkpoint: \nwandb_code_dir: \n</code></pre> <p>Where you can see we can group and order config components and define several high level attributes such as the checkpoint directory. You may like, however, to change these. For example including <code>wandb_project</code> will assume you can <code>wandb.login()</code> (see the wandb website for information on wandb and how to get a free account) and set this as the project. </p> <p>The <code>wandb_code_dir</code> argument will be used later to log the specific code used. </p> <p>For some files, a field may read: <code>???</code> indicating that field must be filled or <code>hydra</code> will error. </p> <p>Overall, fields in config files are accessible as <code>.[attribute]</code> in the DictConfig (from Omegaconf) object for example <code>config.model.n_genes</code>.</p> <p>Keep in mind that for dataset paths, all of them ought to be hardcoded in. Therefore, for datapaths in <code>config/data</code> these paths should be considered placeholders for you to fill in. I left in paths to explicitly indicate filepaths to the Zhuang and AIBS MERFISH data hosted on https://allen-brain-cell-atlas.s3.us-west-2.amazonaws.com/index.html.</p>"},{"location":"#training-on-the-aibs-merfish-data","title":"Training on the AIBS MERFISH data","text":"<p>The entrypoint to the training used for the Allen Institute for Brain Science MERFISH dataset (mouse 6388550) is in <code>scripts/train_aibs_mouse.py</code>, which uses <code>scripts/config/aibs1.yaml</code>. To run the code (assuming the package has been installed):</p> <ol> <li>download the data (use <code>scripts/download_aibs.sh</code>)</li> <li>edit <code>config/data/mouse1.yaml</code>, specifically: <pre><code>adata_path: './abc_dataset/C57BL6J-6388550-log2.h5ad'\nmetadata_path: './abc_dataset/cell_metadata_with_cluster_annotation.csv'\n</code></pre></li> <li>change whatever combination of checkpoint and <code>wandb</code> settings in <code>scripts/config/aibs1.yaml</code></li> <li>run the trainer script (<code>scripts/training/train_aibs_mouse.py</code>)</li> </ol> <pre><code>chmod +x scripts/download_aibs.sh\n./scripts/download_aibs.sh\npython scripts/training/train_aibs_mouse.py\n</code></pre> <p>If this is useful to you, please consider citing our preprint:</p> <pre><code>@article {Lee2024.05.05.592608,\n    author = {Lee, Alex J. and Yao, Shenqin and Lusk, Nicholas and Ng, Lydia and Kunst, Michael and Zeng, Hongkui and Tasic, Bosiljka and Abbasi-Asl, Reza},\n    title = {Data-driven fine-grained region discovery in the mouse brain with transformers},\n    elocation-id = {2024.05.05.592608},\n    year = {2024},\n    doi = {10.1101/2024.05.05.592608},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {Technologies such as spatial transcriptomics offer unique opportunities to define the spatial organization of the mouse brain. We developed an unsupervised training scheme and novel transformer-based deep learning architecture to detect spatial domains across the whole mouse brain using spatial transcriptomics data. Our model learns local representations of molecular and cellular statistical patterns which can be clustered to identify spatial domains within the brain from coarse to fine-grained. Discovered domains are spatially regular, even with several hundreds of spatial clusters. They are also consistent with existing anatomical ontologies such as the Allen Mouse Brain Common Coordinate Framework version 3 (CCFv3) and can be visually interpreted at the cell type or transcript level. We demonstrate our method can be used to identify previously uncatalogued subregions, such as in the midbrain, where we uncover gradients of inhibitory neuron complexity and abundance. Notably, these subregions cannot be discovered using other methods. We apply our method to a separate multi-animal whole-brain spatial transcriptomic dataset and show that our method can also robustly integrate spatial domains across animals.Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2024/06/13/2024.05.05.592608},\n    eprint = {https://www.biorxiv.org/content/early/2024/06/13/2024.05.05.592608.full.pdf},\n    journal = {bioRxiv}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This documentation was copied from Patrick Kidger's <code>jaxtyping</code> docs.</p>"},{"location":"analysis/","title":"post-hoc analysis","text":"<p>See scripts <code>scripts/cluster.py</code> and <code>scripts/embeds_from_files.py</code>.</p> <p>Once the model is trained, see <code>embeds_from_files.py</code> for a minimal example of extracting embeddings from the AIBS data. </p> <p>K-means clustering can be run with example code at <code>cluster.py</code>. </p>"},{"location":"data/","title":"data + dataloader","text":""},{"location":"data/#dataloader-information","title":"Dataloader information","text":"<p>We use a packed sequence format, so the returns from this function are not in general <code>(batch_size, length, features)</code> tensor but a <code>(length, features)</code> matrix. </p> <p>Here is the docstring for the loader:</p> <p>Sampler that returns the gene expression matrices and cell-type identity vectors  for two groups of cells: the observed cells and the masked/reference cells. The observed cells  are the cells in the neighborhood of the reference cell (set by <code>patch_size</code>).</p> <p>We assume the type will be found at <code>cell_type_colname</code> and that <code>cell_id</code> can be used  succesfully to index into the adata object.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.</p> required <code>adata</code> <code>AnnData</code> <p>Expression-containing (as .X) anndata. We assume input will be log scaled.</p> required <code>patch_size</code> <code>Union[List[int], Tuple[int]]</code> <p>Size in arbitrary units for the neighborhood calculation.</p> required <code>cell_id_colname</code> <code>Optional[str]</code> <p>The column to use to index into the anndata, by default \"cell_label\"</p> <code>'cell_label'</code> <code>cell_type_colname</code> <code>Optional[str]</code> <p>The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"</p> <code>'cell_type'</code> <code>tissue_section_colname</code> <code>Optional[str]</code> <p>To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"</p> <code>'brain_section_label'</code> <code>max_num_cells</code> <code>Optional[Union[int, None]]</code> <p>How many cells to threshold at for the neighborhood size, by default None</p> <code>None</code> <code>indices</code> <code>Optional[Union[List[int], None]]</code> <p>Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with  <code>.iloc</code>, so it may be advisable to reset the index of the dataframe. By default None</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If adata is not an <code>ad.annData</code> object</p> <code>ValueError</code> <p>If metadata does not contain the necessary columns</p> <code>TypeError</code> <p>If metadata is not a pandas DataFrame</p> <code>ValueError</code> <p>If patch_size is not a tuple of length 2</p> <code>ValueError</code> <p>If metadata and adata are not the same length</p> <code>ValueError</code> <p>If metadata does not contain columns \"x\" and \"y\"</p> <p>The required data components are an <code>anndata</code> object with keys corresponding to columns in a dataframe of metadata, <code>metadata</code>. </p> <p>The dataframe must have:</p> <ul> <li>x: spatial coordinates [default: x]</li> <li>y: spatial coordinates [default: y]</li> <li>one column (<code>cell_id_colname</code>) which refers to the keys, these will be used directly to subset into the <code>anndata</code> object [default: cell_label]</li> <li>one column (<code>cell_type_colname</code>) which is a class encoded integer corresponding to the single cell classes from the non-spatial scRNA-seq data clustering [default: cell_type]</li> </ul> <p>The units of <code>patch_size</code> are not scaled internally, so they must \"match\" the units of <code>x</code> and <code>y</code>. </p> <p>What if you don't have a dataframe with those columns (and therefore is incorrectly formatted for <code>brainformr.data.CenterMaskSampler</code>)? Your options are:</p> <ol> <li>just create a new version of the dataframe with the correct column names and metadata </li> <li>use the <code>scripts/training/lightning_model.py:BaseTrainer.load_data</code> function to preprocess your data in the format that will satisfy the conditions (see <code>scripts/training/train_aibs_mouse.py:load_data</code>) as an example. The motivations for this are covered in the TLDR in the main page, but we will also discuss it here. </li> </ol>"},{"location":"data/#expectations-for-inputs-into-brainformrdatacentermasksampler-using-scriptstrainingtrain_aibs_mousepyload_data-as-an-example","title":"Expectations for inputs into <code>brainformr.data.CenterMaskSampler</code>, using <code>scripts/training/train_aibs_mouse.py:load_data</code> as an example","text":"<ol> <li>we need columns <code>x</code> and <code>y</code> which match the units of <code>patch_size</code><ul> <li>e.g. if your patch size desired size is 10um, and your spatial dimensions are provided in nm, you would want to rescale either the patch size or the spatial dimensions. I elected to primarily rescale the spatial dimensions (for my own sanity) all to um, and so use the <code>load_data</code> function do do so.</li> </ul> </li> <li>we also need maybe to remap <code>cell_type_colname</code> to give </li> </ol> <p>Therefore, let's annotate the test code from <code>train_aibs_mouse.py:load_data</code>:</p> <pre><code># make sure to encode as str because for whatever reason\n# in the AIBS data, the anndata row ID's are string instead of int\nmetadata[\"cell_label\"] = metadata[\"cell_label\"].astype(str) \nmetadata[\"x\"] = metadata[\"x_reconstructed\"] * 100 # initially x_reconstructed in wrong units\nmetadata[\"y\"] = metadata[\"y_reconstructed\"] * 100\n\nmetadata = metadata[ # throw away nonessential columns\n    [\n        config.data.celltype_colname,\n        \"cell_label\", \n        \"x\",\n        \"y\",\n        \"brain_section_label\",\n    ]\n]\n\n# label_to_cls is just a thin wrapper over \n# `sklearn.preprocessing.LabelEncoder`\nmetadata[\"cell_type\"] = self.label_to_cls(\n    metadata[config.data.celltype_colname]\n)\n# make sure to integer encoder the classes \nmetadata[\"cell_type\"] = metadata[\"cell_type\"].astype(int)\n\nmetadata = metadata.reset_index(drop=True)\n\n# in the AIBS dataset some cells for which gene \n# expression was measured do not have metadata associated\n# so let's throw those out\nadata = adata[metadata[\"cell_label\"]]\n</code></pre> <p></p>"},{"location":"data/#outputs-of-__getitem__-and-collate","title":"Outputs of <code>__getitem__</code> and <code>collate</code>","text":"<p>These are the steps we implement so far:</p> <ol> <li>extract spatial neighbors (within some radius) for the cell of interest</li> <li>extract expression (from the <code>anndata</code> object) and the class encoding (<code>cell_type</code>) for each cell from the metadata dataframe you should have passed to <code>CenterMaskSampler</code></li> <li>partition them into two sets, which are simply lists in a <code>namedtuple</code>, see <code>brainformr.data.NeighborhoodMetadata</code></li> </ol> <p>Note that we then need to stack these together across batches and create attention matrix masks, which is done in <code>brainformr.data.loader_pandas.collate</code>. We create three attention mask matrices (we use a binary adjacency matrix):</p> Syntax Description <code>encoder_mask</code> allow the neighborhoods to only attend to each other <code>pooling_mask</code> pool into a single query token <code>decoder_mask</code> allow query token and decoding cell tokens to attend to each other <p>See the function documentation and the next section model for more information. </p> <p>My first-shot implementation of this workflow (in <code>pandas</code>) was ~1.3X faster than my next attempt, in <code>polars</code>. I welcome any feedback on why my <code>polars</code> code may have been suboptimal!</p>"},{"location":"model/","title":"model attention matrix format","text":""},{"location":"model/#formatting-data-for-the-model","title":"Formatting data for the model:","text":"<p>The model expects a packed sequence format. Because of the various operations of the network, in practice we need three attention mask matrices.</p> <ol> <li> <p>one matrix to mask the encoder </p> <ul> <li>at this stage, the model sees the \"observed\" cells (not the reference cell) and its various <code>&lt;cls&gt;</code>-like tokens</li> <li>we store these in blocks such that the first block of tokens (<code>n_toks</code> by <code>embed_dim</code>) are all the cells in the different neighborhoods, and the last <code>n</code> indices correspond to the -like tokens. Therefore the attention mask for this stage features blocks in the upper left and then spans in the bottom right that allow the <code>&lt;cls&gt;</code>-like tokens to attend to the cell tokens and vice versa. <p>Here is an example of this matrix:</p> <p></p> <p>In this batch of data we can see that the first 15 or so tokens correspond to the first neighborhood. The next 15-20 are in another neighborhood. Then we have a larger neighborhood which has almost 35-40 cells. </p> <p>The smaller lines allow the neighborhood tokens and the <code>&lt;cls&gt;</code>-like tokens to attend to each other. </p> <li> <p>When we pool the tokens together, we use a different mask:</p> <p></p> <ul> <li>since we have three neighborhoods, we have three pooling tokens that are True/1 (or False/0, depending on the coding of the attention matrix masking) in spans corresponding to the neighborhood identity.</li> </ul> </li> <li> <p>Finally, we then concatenate these to the decoding tokens corresponding to the reference cells; for this we need a third matrix:</p> <p></p> <p>Where you can see the first token (the first pooled neighborhood representation) and the fourth (the decoding token for the first neighborhood) can attend to each other, and vice-versa.</p> </li> <p>Otherwise the components are all standard <code>PyTorch</code>. This is likely not computationally optimal. If possible future iterations of this codebase ought to include more performant versions of the standard transformer operations.</p>"},{"location":"usage/","title":"Example training on Allen Institute for Brain Science MERFISH dataset","text":""},{"location":"usage/#dataset-details-and-dataloader-setup","title":"Dataset details and dataloader setup","text":""},{"location":"usage/#model-setup","title":"Model setup","text":""},{"location":"usage/#_1","title":"Example training on Allen Institute for Brain Science MERFISH dataset","text":""},{"location":"usage/#brainformr.model.base.CellTransformer","title":"<code>CellTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/base.py</code> <pre><code>class CellTransformer(nn.Module):\n    def __init__(\n        self,\n        encoder_embedding_dim: int,\n        encoder_num_heads: int,\n        encoder_depth: int,\n        decoder_embedding_dim: int,\n        decoder_num_heads: int,\n        decoder_depth: int,\n        attn_pool_heads: Optional[int] = 8,\n        cell_cardinality: Optional[int] = 1024,\n        put_device: Optional[str] = \"cuda\",\n        eps: float = 1e-15,\n        n_genes: Optional[int] = 500,\n        xformer_dropout: Optional[float] = 0.0,\n        bias: Optional[bool] = False,\n        zero_attn: Optional[bool] = True,\n    ):\n        \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n        Parameters\n        ----------\n        encoder_embedding_dim : int\n        encoder_num_heads : int\n        encoder_depth : int\n        decoder_embedding_dim : int\n        decoder_num_heads : int\n        decoder_depth : int\n        attn_pool_heads : Optional[int], optional\n            Number attention pool heads, by default 8\n        cell_cardinality : Optional[int], optional\n            Cardinality/number of different cell types for embedding, by default 1024\n        put_device : Optional[str], optional\n            For later referencing where to put input tensors, by default 'cuda'\n        eps : float, optional\n            Stability additional constant for NB params, by default 1e-15\n        n_genes : Optional[int], optional\n            Number genes, by default 500\n        xformer_dropout : Optional[float], optional\n            Dropout %, by default 0.0\n        bias : Optional[bool], optional\n            Use bias or not, by default True\n        zero_attn : Optional[bool], optional\n            Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n        \"\"\"\n        super().__init__()\n\n        self.eps = eps\n\n        _feature_dim = encoder_embedding_dim // 2\n\n        self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n        self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n        self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n        self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n        self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n        self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n        self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                                  bias=True, zero_attn=True)\n        # no bias causes instability in training\n\n        self.encoder = set_up_transformer_layers(\n            encoder_embedding_dim,\n            encoder_num_heads,\n            encoder_depth,\n            xformer_dropout,\n            bias,\n            zero_attn,\n        )\n        #self.encoder = torch.compile(self.encoder)\n\n        self.decoder = set_up_transformer_layers(\n            decoder_embedding_dim,\n            decoder_num_heads,\n            decoder_depth,\n            xformer_dropout,\n            bias,\n            zero_attn,\n        )\n        #self.decoder = torch.compile(self.decoder)\n\n        self.zinb_proj = ZINBProj(\n            embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n        )\n\n        nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n        self.put_device = put_device\n\n    def forward(self, data_dict: dict):\n        bs = data_dict[\"bs\"]\n\n        cells = data_dict[\"observed_cell_type\"].to(self.put_device, non_blocking=False)\n        expression = data_dict[\"observed_expression\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        num_hidden = len(\n            data_dict[\"masked_cell_type\"]\n        )  # will in practice be == bs but in case later want to train on multiple cells\n        hidden_expression = data_dict[\"masked_expression\"].to(\n            self.put_device, non_blocking=False, dtype=torch.float32\n        )\n        hidden_cells = data_dict[\"masked_cell_type\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        pooling_mask = data_dict[\"pooling_mask\"].to(self.put_device, non_blocking=False)\n        decoder_mask = data_dict[\"decoder_mask\"].to(self.put_device, non_blocking=False)\n        encoder_mask = data_dict[\"encoder_mask\"].to(self.put_device, non_blocking=False)\n\n        cls_tokens = self.cls_token.repeat_interleave(bs, dim=0)\n\n        cells_embed = self.encoder_cell_embed(cells)\n\n        expression_embed = self.expression_projection(expression)\n        cells_embed = torch.cat((cells_embed, expression_embed), dim=1)\n        cells_embed = torch.cat((cells_embed, cls_tokens), dim=0)\n\n        cells_embed = self.proj_norm(cells_embed)\n\n        cells_embed = self.encoder(cells_embed, mask=encoder_mask)\n\n        attn_pool = self.attn_pool(cells_embed, pooling_mask, bs)\n\n        decoding_queries = self.decoder_cell_embed(hidden_cells)\n\n        cells_embed = torch.cat((attn_pool, decoding_queries), dim=0)\n\n        cells_embed = self.decoder(cells_embed, mask=decoder_mask)\n        ref_cell_embed = cells_embed[-bs:]\n\n        with torch.autocast(dtype=torch.float32, device_type=self.put_device):\n            # sometimes bfloat doesn't work here for lgamma backward (in zinb, otherwise will error)\n            zinb_params = self.zinb_proj(ref_cell_embed)\n\n        cls_toks = cells_embed[-(num_hidden + bs) : -num_hidden]\n\n        return dict(\n            zinb_params=zinb_params,\n            neighborhood_repr=cls_toks,\n            hidden_expression=hidden_expression,\n        )\n</code></pre>"},{"location":"usage/#brainformr.model.base.CellTransformer.__init__","title":"<code>__init__(encoder_embedding_dim, encoder_num_heads, encoder_depth, decoder_embedding_dim, decoder_num_heads, decoder_depth, attn_pool_heads=8, cell_cardinality=1024, put_device='cuda', eps=1e-15, n_genes=500, xformer_dropout=0.0, bias=False, zero_attn=True)</code>","text":"<p>An encoder-decoder model with attention pooling prior to decoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_embedding_dim</code> <code>int</code> required <code>encoder_num_heads</code> <code>int</code> required <code>encoder_depth</code> <code>int</code> required <code>decoder_embedding_dim</code> <code>int</code> required <code>decoder_num_heads</code> <code>int</code> required <code>decoder_depth</code> <code>int</code> required <code>attn_pool_heads</code> <code>Optional[int]</code> <p>Number attention pool heads, by default 8</p> <code>8</code> <code>cell_cardinality</code> <code>Optional[int]</code> <p>Cardinality/number of different cell types for embedding, by default 1024</p> <code>1024</code> <code>put_device</code> <code>Optional[str]</code> <p>For later referencing where to put input tensors, by default 'cuda'</p> <code>'cuda'</code> <code>eps</code> <code>float</code> <p>Stability additional constant for NB params, by default 1e-15</p> <code>1e-15</code> <code>n_genes</code> <code>Optional[int]</code> <p>Number genes, by default 500</p> <code>500</code> <code>xformer_dropout</code> <code>Optional[float]</code> <p>Dropout %, by default 0.0</p> <code>0.0</code> <code>bias</code> <code>Optional[bool]</code> <p>Use bias or not, by default True</p> <code>False</code> <code>zero_attn</code> <code>Optional[bool]</code> <p>Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True</p> <code>True</code> Source code in <code>brainformr/model/base.py</code> <pre><code>def __init__(\n    self,\n    encoder_embedding_dim: int,\n    encoder_num_heads: int,\n    encoder_depth: int,\n    decoder_embedding_dim: int,\n    decoder_num_heads: int,\n    decoder_depth: int,\n    attn_pool_heads: Optional[int] = 8,\n    cell_cardinality: Optional[int] = 1024,\n    put_device: Optional[str] = \"cuda\",\n    eps: float = 1e-15,\n    n_genes: Optional[int] = 500,\n    xformer_dropout: Optional[float] = 0.0,\n    bias: Optional[bool] = False,\n    zero_attn: Optional[bool] = True,\n):\n    \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n    Parameters\n    ----------\n    encoder_embedding_dim : int\n    encoder_num_heads : int\n    encoder_depth : int\n    decoder_embedding_dim : int\n    decoder_num_heads : int\n    decoder_depth : int\n    attn_pool_heads : Optional[int], optional\n        Number attention pool heads, by default 8\n    cell_cardinality : Optional[int], optional\n        Cardinality/number of different cell types for embedding, by default 1024\n    put_device : Optional[str], optional\n        For later referencing where to put input tensors, by default 'cuda'\n    eps : float, optional\n        Stability additional constant for NB params, by default 1e-15\n    n_genes : Optional[int], optional\n        Number genes, by default 500\n    xformer_dropout : Optional[float], optional\n        Dropout %, by default 0.0\n    bias : Optional[bool], optional\n        Use bias or not, by default True\n    zero_attn : Optional[bool], optional\n        Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n    \"\"\"\n    super().__init__()\n\n    self.eps = eps\n\n    _feature_dim = encoder_embedding_dim // 2\n\n    self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n    self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n    self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n    self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n    self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n    self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n    self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                              bias=True, zero_attn=True)\n    # no bias causes instability in training\n\n    self.encoder = set_up_transformer_layers(\n        encoder_embedding_dim,\n        encoder_num_heads,\n        encoder_depth,\n        xformer_dropout,\n        bias,\n        zero_attn,\n    )\n    #self.encoder = torch.compile(self.encoder)\n\n    self.decoder = set_up_transformer_layers(\n        decoder_embedding_dim,\n        decoder_num_heads,\n        decoder_depth,\n        xformer_dropout,\n        bias,\n        zero_attn,\n    )\n    #self.decoder = torch.compile(self.decoder)\n\n    self.zinb_proj = ZINBProj(\n        embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n    )\n\n    nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n    self.put_device = put_device\n</code></pre>"},{"location":"zhuang_abc/","title":"Training on Zhuang lab data","text":"<p>In order to train on the Zhuang lab data please see the config files labeled \"Zhuang\".</p> <p>The setup is more or less the same as for the AIBS case. One easy solution is to simply load each metadata / <code>anndata</code> object indepently as a <code>brainformr.data.SimpleMaskSampler</code> object and concatenate them together post-hoc using <code>torch.utils.data.ConcatDataset</code>. This is what we do.</p> <p>For reference, here is a snippet of code from the <code>ZhuangTrainer.load_data</code> function (see <code>scripts/training/train_zhuang.py</code>) and some annotation by me:</p> <pre><code>def load_data(self, config: DictConfig):\n\n    all_dfs = []\n    all_cls = set()\n\n    for df_path in config.data.metadata_path: \n\n        # We loop over all the dataframes first, so we can generate a consistent list \n        of all the celltypes in the datasets.\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", pd.errors.DtypeWarning)\n            metadata = pd.read_csv(df_path)\n\n        metadata[\"x\"] = metadata[\"x\"] * 100\n        metadata[\"y\"] = metadata[\"y\"] * 100\n\n        metadata = metadata.reset_index(drop=True)\n\n        all_cls.update(metadata[config.data.celltype_colname].unique())\n        all_dfs.append(metadata)\n\n    le = LabelEncoder()\n    le.fit(sorted(all_cls))\n\n\n    # Now that we have this (`le` consistent encoder) we can use this as we loop again \n    over the metadata/anndata pairs, creating for each one a `CenterMaskSampler` pair \n    and appending to the `trn_samplers` and `valid_samplers` lists.\n\n    trn_samplers = []\n    valid_samplers = []\n\n    for df, anndata_path in zip(all_dfs, config.data.adata_path):\n        df[\"cell_type\"] = le.transform(df[config.data.celltype_colname])\n        df[\"cell_type\"] = df[\"cell_type\"].astype(int)\n\n        df['cell_label'] = df['cell_label'].astype(str)\n\n        df = df[['cell_type', 'cell_label', 'x', 'y', 'brain_section_label']]\n\n        adata = ad.read_h5ad(anndata_path)\n        adata = adata[df[\"cell_label\"]]\n\n        train_indices, valid_indices = train_test_split(\n            range(len(adata)), train_size=config.data.train_pct\n        )\n\n        train_sampler = CenterMaskSampler(\n            metadata=df,\n            adata=adata,\n            patch_size=config.data.patch_size,\n            cell_id_colname=config.data.cell_id_colname,\n            cell_type_colname=\"cell_type\",\n            tissue_section_colname=config.data.tissue_section_colname,\n            max_num_cells=config.data.neighborhood_max_num_cells,\n            indices=train_indices,\n        )\n\n        valid_sampler = CenterMaskSampler(\n            metadata=df,\n            adata=adata,\n            patch_size=config.data.patch_size,\n            cell_id_colname=config.data.cell_id_colname,\n            cell_type_colname=\"cell_type\",\n            tissue_section_colname=config.data.tissue_section_colname,\n            max_num_cells=config.data.neighborhood_max_num_cells,\n            indices=valid_indices,\n        )\n\n        trn_samplers.append(train_sampler)\n        valid_samplers.append(valid_sampler)\n\n    train_loader = torch.utils.data.DataLoader(\n        torch.utils.data.ConcatDataset(trn_samplers),\n        batch_size=config.data.batch_size,\n        num_workers=config.data.num_workers,\n        pin_memory=False,\n        shuffle=True, \n        collate_fn=collate,\n        prefetch_factor=4, # muddling with this a bit can improve performance, depends on your setup\n\n    )\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>analysis_utils<ul> <li>crosscorr</li> </ul> </li> <li>data<ul> <li>_loader_polars</li> <li>attn_loader</li> <li>loader_pandas</li> </ul> </li> <li>model<ul> <li>base</li> <li>blocks</li> <li>ct</li> <li>factory</li> </ul> </li> <li>training<ul> <li>scheduler</li> </ul> </li> </ul>"},{"location":"reference/analysis_utils/","title":"analysis_utils","text":""},{"location":"reference/analysis_utils/crosscorr/","title":"crosscorr","text":""},{"location":"reference/analysis_utils/crosscorr/#analysis_utils.crosscorr.corr_predictions","title":"<code>corr_predictions(a, b)</code>","text":"<p>Computes the cross correlation between two pairs of matrices of the same length. Instantiating the entire matrix is unnecessary, so we just compute the pairwise  distances between the (i,i) entries in the two matrices.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Float[Tensor, 'ncells ngenes']</code> <p>obs by features matrix</p> required <code>b</code> <code>Float[Tensor, 'ncells ngenes']</code> <p>obs by features matrix; checked to be same size as <code>a</code></p> required <p>Returns:</p> Type Description <code>Float[Tensor, ncells]</code> <p>Cross correlations where each element is Pearson(a_i, b_i) for i in num_obs.</p> Source code in <code>brainformr/analysis_utils/crosscorr.py</code> <pre><code>def corr_predictions(\n    a: Float[torch.Tensor, \"ncells ngenes\"], b: Float[torch.Tensor, \"ncells ngenes\"]    # noqa: F722\n) -&gt; Float[torch.Tensor, \"ncells\"]:    # noqa: F821\n    \"\"\"\n    Computes the cross correlation between two pairs of matrices of the same length.\n    Instantiating the entire matrix is unnecessary, so we just compute the pairwise \n    distances between the (i,i) entries in the two matrices.\n\n    Parameters\n    ---------\n    a: Float[torch.Tensor, \"ncells ngenes\"] \n        obs by features matrix\n    b: Float[torch.Tensor, \"ncells ngenes\"] \n        obs by features matrix; checked to be same size as `a`\n\n    Returns\n    -------\n    Float[torch.Tensor, \"ncells\"] \n        Cross correlations where each element is Pearson(a_i, b_i) for i in num_obs.\n    \"\"\"\n\n    assert a.shape == b.shape, \"a and b must have the same shape\"\n    n_el = a.shape[1]\n\n    a = normalize_matrix(a)\n    b = normalize_matrix(b)\n\n    sigma_a = a.std(1)\n    sigma_b = b.std(1)\n\n    # ommitting .item() will cause a shape error if args are torch.Tensor\n    # otherwise this code will also work for numpy arrays\n    if sigma_a.size == 1:\n        sigma_a = sigma_a.item()\n\n    if sigma_b.size == 1:\n        sigma_b = sigma_b.item()\n\n    prod = sigma_a * sigma_b\n\n    cov = (a * b).sum(1) / n_el\n    return cov / prod\n</code></pre>"},{"location":"reference/data/","title":"data","text":""},{"location":"reference/data/#data.CenterMaskSampler","title":"<code>CenterMaskSampler</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>class CenterMaskSampler(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        metadata: pd.DataFrame,\n        adata: ad.AnnData,\n        patch_size: Union[List[int], Tuple[int]],\n        cell_id_colname: Optional[str] = \"cell_label\",\n        cell_type_colname: Optional[str] = \"cell_type\",\n        tissue_section_colname: Optional[str] = \"brain_section_label\",\n        max_num_cells: Optional[Union[int, None]] = None,\n        indices: Optional[Union[List[int], None]] = None,\n    ):\n        \"\"\"Sampler that returns the gene expression matrices and cell-type identity vectors\n         for two groups of cells: the observed cells and the masked/reference cells. The observed cells\n         are the cells in the neighborhood of the reference cell (set by `patch_size`).\n\n         We assume the type will be found at `cell_type_colname` and that `cell_id` can be used\n         succesfully to index into the adata object.\n\n        Parameters\n        ----------\n        metadata : pd.DataFrame\n            Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.\n        adata : ad.AnnData\n            Expression-containing (as .X) anndata. We assume input will be log scaled.\n        patch_size : Union[List[int], Tuple[int]]\n            Size in arbitrary units for the neighborhood calculation.\n        cell_id_colname : Optional[str], optional\n            The column to use to index into the anndata, by default \"cell_label\"\n        cell_type_colname : Optional[str], optional\n            The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"\n        tissue_section_colname : Optional[str], optional\n            To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"\n        max_num_cells : Optional[Union[int, None]], optional\n            How many cells to threshold at for the neighborhood size, by default None\n        indices: Optional[Union[List[int], None]], optional\n            Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with \n            `.iloc`, so it may be advisable to reset the index of the dataframe. By default None\n\n        Raises\n        ------\n        TypeError\n            If adata is not an `ad.annData` object\n        ValueError\n            If metadata does not contain the necessary columns\n        TypeError\n            If metadata is not a pandas DataFrame\n        ValueError\n            If patch_size is not a tuple of length 2\n        ValueError\n            If metadata and adata are not the same length\n        ValueError\n            If metadata does not contain columns \"x\" and \"y\"\n        \"\"\"\n        if isinstance(adata, ad.AnnData) is False:\n            raise TypeError(\n                f\"Input argument adata must be of type np.ndarray; got {type(adata)}.\"\n            )\n\n        if not all(\n            (\n                cell_type_colname in metadata.columns,\n                cell_id_colname in metadata.columns,\n                tissue_section_colname in metadata.columns,\n            )\n        ):\n            raise ValueError(\n                \"Provided metadata has to have columns with columns specified by cell_id_colname, cell_type_colname and tissue_section_colname.\"\n            )\n\n        if isinstance(metadata, pd.DataFrame) is False:\n            raise TypeError(\n                f\"Input argument metadata must be of type pl.DataFrame or pd.DataFrame; got {type(metadata)}.\"\n            )\n\n        if len(patch_size) != 2:\n            raise ValueError(\n                f\"Input argument patch_size must be a tuple of length 2 with desired dims order (x, y, z); got {len(patch_size)}.\"\n            )\n\n        if len(metadata) != len(adata):\n            raise ValueError(\"Metadata and adata must have the same length.\")\n\n        if not (\"x\" in metadata.columns and \"y\" in metadata.columns):\n            raise ValueError(\n                \"Metadata must contain columns named x and y to index the spatial coordinates of the cells.\"\n            )\n\n        self.metadata = metadata\n\n        # use these later to index into metadata dataframe\n        self.cell_type_colname = cell_type_colname\n        self.cell_id_colname = cell_id_colname\n        self.tissue_section_colname = tissue_section_colname\n        self.length = len(self.metadata) if indices is None else len(indices)\n        self.max_num_cells = np.inf if max_num_cells is None else max_num_cells\n\n        self.adata = adata\n        self.patch_size = np.array(patch_size).astype(int)\n        #self.noise_fac = 2\n\n        self._preprocess_metadata()\n\n        # so that later we can simply pass a train/valid/test set of indices\n        # instead of filtering up front\n        self.indices = indices if indices is not None else list(range(self.length))\n\n    def __len__(self):\n        return self.length\n\n    def _preprocess_metadata(self):\n        # for use in reducing complexity of neighborhood queries in getitem\n        self.tissue_section_mapper = {\n            section_label: subset_df\n            for section_label, subset_df in self.metadata.groupby(\n                self.tissue_section_colname\n            )\n        }\n\n    def get_nearby_cells(\n        self,\n        centroid: npt.ArrayLike,\n        patch_size: npt.ArrayLike,\n        tissue_section: pd.DataFrame,\n        discretize: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Return the cells from tissue_section that are within\n        patch_size distance away from centroid.\n\n        Parameters\n        ----------\n        centroid : npt.ArrayLike\n            A 1x2 vector representing the centroid of the patch.\n        patch_size : npt.ArrayLike\n            A 1x2 vector representing the size of the patch.\n        tissue_section : pd.DataFrame\n            A pandas DataFrame containing the cells in the tissue section.\n            It's assumed that the cell corresponding to centroid is in this dataframe.\n        discretize : bool, optional\n            _description_, by default False\n\n        Returns\n        -------\n        pd.DataFrame\n            Cells inside the neighborhood, inclusive of the\n            centroid cell / reference cell.\n        \"\"\"\n        start = centroid - (patch_size / 2)\n        if discretize:\n            start = np.rint(start).astype(int)\n\n        end = start + patch_size\n\n        lookup_x = tissue_section[\"x\"].between(start[0], end[0])\n        lookup_y = tissue_section[\"y\"].between(start[1], end[1])\n\n        return tissue_section[lookup_x &amp; lookup_y]\n\n    def idx_to_expr(self, indices: Union[str, int, List[Union[str, int]]]):\n        return self.adata[indices].X\n\n    def __getitem__(self, index) -&gt; dict:\n        orig_idx = self.indices[index]  # see init for expln\n        metadata_row = self.metadata.iloc[orig_idx]\n        index = metadata_row[self.cell_id_colname]\n\n        centroid = np.array([metadata_row[\"x\"], metadata_row[\"y\"]])\n#        centroid = add_gaussian(centroid, self.noise_fac)\n\n        tissue_section: pd.DataFrame = self.tissue_section_mapper.get(\n            metadata_row[self.tissue_section_colname], None\n        )\n\n        assert tissue_section is not None, (\n            \"Tissue section not found in `tissue_section_mapper` dict. \"\n            \"Check that the tissue_section_colname is correctly set in the metadata.\"\n        )\n\n        all_neighborhood_cells = self.get_nearby_cells(\n            centroid, self.patch_size, tissue_section, discretize=True\n        )\n\n        if len(all_neighborhood_cells) == 1:\n            # manually create an empty neighborhood\n            masked_expression: Float[np.array, \"one n_genes\"] = self.idx_to_expr(index)  # noqa: F722\n            # check if is csr, if so convert\n            if not isinstance(masked_expression, np.ndarray):\n                masked_expression = masked_expression.toarray()\n            masked_cell_type: int = metadata_row[self.cell_type_colname]\n\n            neighborhood_metadata = NeighborhoodMetadata(\n                observed_expression=torch.FloatTensor([]),\n                masked_expression=torch.from_numpy(masked_expression),\n                observed_cell_type=torch.LongTensor([]),\n                masked_cell_type=torch.LongTensor([masked_cell_type]),\n                num_cells_obs=0,\n            )\n            return neighborhood_metadata\n\n        return self.neighborhood_gather(all_neighborhood_cells, index, as_dict=False)\n\n    def neighborhood_gather(\n        self,\n        neighborhood_cells: pd.DataFrame,\n        ref_cell_label: str,\n        as_dict: Optional[bool] = False,\n    ):\n        \"\"\"Get the neighborhood cells' metadata and partition them into two sets.\n        In this case we will only predict the center / reference cell.\n\n        Parameters\n        ----------\n        neighborhood_cells : pd.DataFrame\n            Dataframe containing the cells in the neighborhood. One\n            cell's metadata per row.\n        ref_cell_label : str\n            The label of the reference cell as a string.\n        as_dict : Optional[bool], optional\n            Return a dictionary (True) or NeighborhoodMetadata (False),\n            by default False.\n\n        Returns\n        -------\n        Union[NeighborhoodMetadata, dict]\n            A namedtuple or dictionary containing the partitioned cells'\n            metadata, meaning the expression and class label integer for\n            each of the cells. The number of observed cells is also returned,\n            so in total there will be five entries / keys.\n\n        \"\"\"\n\n        neighborhood_cells = neighborhood_cells.reset_index(drop=True)\n        neighborhood_cells_indices = neighborhood_cells[self.cell_id_colname]\n        expression = self.adata[neighborhood_cells_indices].X\n\n        # check if is csr, if so convert\n        if not isinstance(expression, np.ndarray):\n            expression = expression.toarray()\n\n        num_cells_neighborhood = len(neighborhood_cells)\n\n        observed_cells: pd.DataFrame = neighborhood_cells[\n            neighborhood_cells_indices != ref_cell_label\n        ]\n\n        masked_cell = neighborhood_cells[neighborhood_cells_indices == ref_cell_label]\n\n        if self.max_num_cells &lt; (num_cells_neighborhood - 1):\n            random_indices: List[int] = random_indices_from_series(\n                observed_cells[self.cell_id_colname], self.max_num_cells\n            )\n            observed_cells = observed_cells.iloc[random_indices]\n\n        observed_cell_types = observed_cells[self.cell_type_colname].values\n        observed_expression: Float[np.ndarray, \"cells genes\"] = expression[observed_cells.index] # noqa: F722\n\n        masked_cell_types = masked_cell[self.cell_type_colname].values\n        masked_expression: Float[np.ndarray, \"cells genes\"] = expression[masked_cell.index] # noqa: F722\n\n        num_cells_obs = len(observed_cells)\n\n        if as_dict:\n            return dict(\n                observed_expression=observed_expression,\n                masked_expression=masked_expression,\n                observed_cell_type=observed_cell_types,\n                masked_cell_type=masked_cell_types,\n                num_cells_obs=num_cells_obs,\n            )\n        else:\n            return NeighborhoodMetadata(\n                torch.from_numpy(observed_expression),\n                torch.from_numpy(masked_expression),\n                torch.from_numpy(observed_cell_types),\n                torch.from_numpy(masked_cell_types),\n                num_cells_obs,\n            )\n</code></pre>"},{"location":"reference/data/#data.CenterMaskSampler.__init__","title":"<code>__init__(metadata, adata, patch_size, cell_id_colname='cell_label', cell_type_colname='cell_type', tissue_section_colname='brain_section_label', max_num_cells=None, indices=None)</code>","text":"<p>Sampler that returns the gene expression matrices and cell-type identity vectors  for two groups of cells: the observed cells and the masked/reference cells. The observed cells  are the cells in the neighborhood of the reference cell (set by <code>patch_size</code>).</p> <p>We assume the type will be found at <code>cell_type_colname</code> and that <code>cell_id</code> can be used  succesfully to index into the adata object.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.</p> required <code>adata</code> <code>AnnData</code> <p>Expression-containing (as .X) anndata. We assume input will be log scaled.</p> required <code>patch_size</code> <code>Union[List[int], Tuple[int]]</code> <p>Size in arbitrary units for the neighborhood calculation.</p> required <code>cell_id_colname</code> <code>Optional[str]</code> <p>The column to use to index into the anndata, by default \"cell_label\"</p> <code>'cell_label'</code> <code>cell_type_colname</code> <code>Optional[str]</code> <p>The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"</p> <code>'cell_type'</code> <code>tissue_section_colname</code> <code>Optional[str]</code> <p>To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"</p> <code>'brain_section_label'</code> <code>max_num_cells</code> <code>Optional[Union[int, None]]</code> <p>How many cells to threshold at for the neighborhood size, by default None</p> <code>None</code> <code>indices</code> <code>Optional[Union[List[int], None]]</code> <p>Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with  <code>.iloc</code>, so it may be advisable to reset the index of the dataframe. By default None</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If adata is not an <code>ad.annData</code> object</p> <code>ValueError</code> <p>If metadata does not contain the necessary columns</p> <code>TypeError</code> <p>If metadata is not a pandas DataFrame</p> <code>ValueError</code> <p>If patch_size is not a tuple of length 2</p> <code>ValueError</code> <p>If metadata and adata are not the same length</p> <code>ValueError</code> <p>If metadata does not contain columns \"x\" and \"y\"</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def __init__(\n    self,\n    metadata: pd.DataFrame,\n    adata: ad.AnnData,\n    patch_size: Union[List[int], Tuple[int]],\n    cell_id_colname: Optional[str] = \"cell_label\",\n    cell_type_colname: Optional[str] = \"cell_type\",\n    tissue_section_colname: Optional[str] = \"brain_section_label\",\n    max_num_cells: Optional[Union[int, None]] = None,\n    indices: Optional[Union[List[int], None]] = None,\n):\n    \"\"\"Sampler that returns the gene expression matrices and cell-type identity vectors\n     for two groups of cells: the observed cells and the masked/reference cells. The observed cells\n     are the cells in the neighborhood of the reference cell (set by `patch_size`).\n\n     We assume the type will be found at `cell_type_colname` and that `cell_id` can be used\n     succesfully to index into the adata object.\n\n    Parameters\n    ----------\n    metadata : pd.DataFrame\n        Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.\n    adata : ad.AnnData\n        Expression-containing (as .X) anndata. We assume input will be log scaled.\n    patch_size : Union[List[int], Tuple[int]]\n        Size in arbitrary units for the neighborhood calculation.\n    cell_id_colname : Optional[str], optional\n        The column to use to index into the anndata, by default \"cell_label\"\n    cell_type_colname : Optional[str], optional\n        The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"\n    tissue_section_colname : Optional[str], optional\n        To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"\n    max_num_cells : Optional[Union[int, None]], optional\n        How many cells to threshold at for the neighborhood size, by default None\n    indices: Optional[Union[List[int], None]], optional\n        Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with \n        `.iloc`, so it may be advisable to reset the index of the dataframe. By default None\n\n    Raises\n    ------\n    TypeError\n        If adata is not an `ad.annData` object\n    ValueError\n        If metadata does not contain the necessary columns\n    TypeError\n        If metadata is not a pandas DataFrame\n    ValueError\n        If patch_size is not a tuple of length 2\n    ValueError\n        If metadata and adata are not the same length\n    ValueError\n        If metadata does not contain columns \"x\" and \"y\"\n    \"\"\"\n    if isinstance(adata, ad.AnnData) is False:\n        raise TypeError(\n            f\"Input argument adata must be of type np.ndarray; got {type(adata)}.\"\n        )\n\n    if not all(\n        (\n            cell_type_colname in metadata.columns,\n            cell_id_colname in metadata.columns,\n            tissue_section_colname in metadata.columns,\n        )\n    ):\n        raise ValueError(\n            \"Provided metadata has to have columns with columns specified by cell_id_colname, cell_type_colname and tissue_section_colname.\"\n        )\n\n    if isinstance(metadata, pd.DataFrame) is False:\n        raise TypeError(\n            f\"Input argument metadata must be of type pl.DataFrame or pd.DataFrame; got {type(metadata)}.\"\n        )\n\n    if len(patch_size) != 2:\n        raise ValueError(\n            f\"Input argument patch_size must be a tuple of length 2 with desired dims order (x, y, z); got {len(patch_size)}.\"\n        )\n\n    if len(metadata) != len(adata):\n        raise ValueError(\"Metadata and adata must have the same length.\")\n\n    if not (\"x\" in metadata.columns and \"y\" in metadata.columns):\n        raise ValueError(\n            \"Metadata must contain columns named x and y to index the spatial coordinates of the cells.\"\n        )\n\n    self.metadata = metadata\n\n    # use these later to index into metadata dataframe\n    self.cell_type_colname = cell_type_colname\n    self.cell_id_colname = cell_id_colname\n    self.tissue_section_colname = tissue_section_colname\n    self.length = len(self.metadata) if indices is None else len(indices)\n    self.max_num_cells = np.inf if max_num_cells is None else max_num_cells\n\n    self.adata = adata\n    self.patch_size = np.array(patch_size).astype(int)\n    #self.noise_fac = 2\n\n    self._preprocess_metadata()\n\n    # so that later we can simply pass a train/valid/test set of indices\n    # instead of filtering up front\n    self.indices = indices if indices is not None else list(range(self.length))\n</code></pre>"},{"location":"reference/data/#data.CenterMaskSampler.get_nearby_cells","title":"<code>get_nearby_cells(centroid, patch_size, tissue_section, discretize=False)</code>","text":"<p>Return the cells from tissue_section that are within patch_size distance away from centroid.</p> <p>Parameters:</p> Name Type Description Default <code>centroid</code> <code>ArrayLike</code> <p>A 1x2 vector representing the centroid of the patch.</p> required <code>patch_size</code> <code>ArrayLike</code> <p>A 1x2 vector representing the size of the patch.</p> required <code>tissue_section</code> <code>DataFrame</code> <p>A pandas DataFrame containing the cells in the tissue section. It's assumed that the cell corresponding to centroid is in this dataframe.</p> required <code>discretize</code> <code>bool</code> <p>description, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Cells inside the neighborhood, inclusive of the centroid cell / reference cell.</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def get_nearby_cells(\n    self,\n    centroid: npt.ArrayLike,\n    patch_size: npt.ArrayLike,\n    tissue_section: pd.DataFrame,\n    discretize: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Return the cells from tissue_section that are within\n    patch_size distance away from centroid.\n\n    Parameters\n    ----------\n    centroid : npt.ArrayLike\n        A 1x2 vector representing the centroid of the patch.\n    patch_size : npt.ArrayLike\n        A 1x2 vector representing the size of the patch.\n    tissue_section : pd.DataFrame\n        A pandas DataFrame containing the cells in the tissue section.\n        It's assumed that the cell corresponding to centroid is in this dataframe.\n    discretize : bool, optional\n        _description_, by default False\n\n    Returns\n    -------\n    pd.DataFrame\n        Cells inside the neighborhood, inclusive of the\n        centroid cell / reference cell.\n    \"\"\"\n    start = centroid - (patch_size / 2)\n    if discretize:\n        start = np.rint(start).astype(int)\n\n    end = start + patch_size\n\n    lookup_x = tissue_section[\"x\"].between(start[0], end[0])\n    lookup_y = tissue_section[\"y\"].between(start[1], end[1])\n\n    return tissue_section[lookup_x &amp; lookup_y]\n</code></pre>"},{"location":"reference/data/#data.CenterMaskSampler.neighborhood_gather","title":"<code>neighborhood_gather(neighborhood_cells, ref_cell_label, as_dict=False)</code>","text":"<p>Get the neighborhood cells' metadata and partition them into two sets. In this case we will only predict the center / reference cell.</p> <p>Parameters:</p> Name Type Description Default <code>neighborhood_cells</code> <code>DataFrame</code> <p>Dataframe containing the cells in the neighborhood. One cell's metadata per row.</p> required <code>ref_cell_label</code> <code>str</code> <p>The label of the reference cell as a string.</p> required <code>as_dict</code> <code>Optional[bool]</code> <p>Return a dictionary (True) or NeighborhoodMetadata (False), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[NeighborhoodMetadata, dict]</code> <p>A namedtuple or dictionary containing the partitioned cells' metadata, meaning the expression and class label integer for each of the cells. The number of observed cells is also returned, so in total there will be five entries / keys.</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def neighborhood_gather(\n    self,\n    neighborhood_cells: pd.DataFrame,\n    ref_cell_label: str,\n    as_dict: Optional[bool] = False,\n):\n    \"\"\"Get the neighborhood cells' metadata and partition them into two sets.\n    In this case we will only predict the center / reference cell.\n\n    Parameters\n    ----------\n    neighborhood_cells : pd.DataFrame\n        Dataframe containing the cells in the neighborhood. One\n        cell's metadata per row.\n    ref_cell_label : str\n        The label of the reference cell as a string.\n    as_dict : Optional[bool], optional\n        Return a dictionary (True) or NeighborhoodMetadata (False),\n        by default False.\n\n    Returns\n    -------\n    Union[NeighborhoodMetadata, dict]\n        A namedtuple or dictionary containing the partitioned cells'\n        metadata, meaning the expression and class label integer for\n        each of the cells. The number of observed cells is also returned,\n        so in total there will be five entries / keys.\n\n    \"\"\"\n\n    neighborhood_cells = neighborhood_cells.reset_index(drop=True)\n    neighborhood_cells_indices = neighborhood_cells[self.cell_id_colname]\n    expression = self.adata[neighborhood_cells_indices].X\n\n    # check if is csr, if so convert\n    if not isinstance(expression, np.ndarray):\n        expression = expression.toarray()\n\n    num_cells_neighborhood = len(neighborhood_cells)\n\n    observed_cells: pd.DataFrame = neighborhood_cells[\n        neighborhood_cells_indices != ref_cell_label\n    ]\n\n    masked_cell = neighborhood_cells[neighborhood_cells_indices == ref_cell_label]\n\n    if self.max_num_cells &lt; (num_cells_neighborhood - 1):\n        random_indices: List[int] = random_indices_from_series(\n            observed_cells[self.cell_id_colname], self.max_num_cells\n        )\n        observed_cells = observed_cells.iloc[random_indices]\n\n    observed_cell_types = observed_cells[self.cell_type_colname].values\n    observed_expression: Float[np.ndarray, \"cells genes\"] = expression[observed_cells.index] # noqa: F722\n\n    masked_cell_types = masked_cell[self.cell_type_colname].values\n    masked_expression: Float[np.ndarray, \"cells genes\"] = expression[masked_cell.index] # noqa: F722\n\n    num_cells_obs = len(observed_cells)\n\n    if as_dict:\n        return dict(\n            observed_expression=observed_expression,\n            masked_expression=masked_expression,\n            observed_cell_type=observed_cell_types,\n            masked_cell_type=masked_cell_types,\n            num_cells_obs=num_cells_obs,\n        )\n    else:\n        return NeighborhoodMetadata(\n            torch.from_numpy(observed_expression),\n            torch.from_numpy(masked_expression),\n            torch.from_numpy(observed_cell_types),\n            torch.from_numpy(masked_cell_types),\n            num_cells_obs,\n        )\n</code></pre>"},{"location":"reference/data/#data.collate","title":"<code>collate(batched_metadata)</code>","text":"<p>Collates metadata which is a list of NeighborhoodMetadata namedtuples. One important computation is the attention matrices that the transformer will use. In particular the <code>encoder_mask</code> which is a square matrix with (n_obs_cells + n_cls_tokens [=bs]) length. The <code>pooling_mask</code> is a matrix of shape (n_pooling_tokens [=bs], n_obs_cells + n_cls_tokens) which is used to pool the hidden states. The <code>decoder_mask</code> is a square matrix of shape (n_pooled_tokens [=bs], n_query_tokens [=bs]) which is used to mask the decoding queries. The other items (the expression matrices and integer-encoded cell type vector) are simply concatenated.</p> <p>Parameters:</p> Name Type Description Default <code>batched_metadata</code> <code>NeighborhoodMetadata</code> <p>Contains attributes: observed_expression, masked_expression, observed_cell_type, masked_cell_type, num_cells_obs Expression are float valued (n_cells x genes) matrices and cell types are integer-valued vectors.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the collated metadata and the attention masks.</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def collate(batched_metadata: NeighborhoodMetadata) -&gt; Dict[str, torch.Tensor | int]:\n    \"\"\"Collates metadata which is a list of NeighborhoodMetadata namedtuples.\n    One important computation is the attention matrices that the transformer will use.\n    In particular the `encoder_mask` which is a square matrix with (n_obs_cells + n_cls_tokens [=bs]) length.\n    The `pooling_mask` is a matrix of shape (n_pooling_tokens [=bs], n_obs_cells + n_cls_tokens) which is used to pool the hidden states.\n    The `decoder_mask` is a square matrix of shape (n_pooled_tokens [=bs], n_query_tokens [=bs]) which is used to mask the decoding queries.\n    The other items (the expression matrices and integer-encoded cell type vector) are simply concatenated.\n\n    Parameters\n    ----------\n    batched_metadata : NeighborhoodMetadata\n        Contains attributes: observed_expression, masked_expression, observed_cell_type, masked_cell_type, num_cells_obs\n        Expression are float valued (n_cells x genes) matrices and cell types are integer-valued vectors.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the collated metadata and the attention masks.\n    \"\"\"\n    observed_expression = []\n    masked_expression = []\n    observed_cell_type = []\n    masked_cell_type = []\n    observed_neighboorhood_lens = []\n\n    tot_num_obs_cells = sum([metadata.num_cells_obs for metadata in batched_metadata])\n    bs = len(batched_metadata)\n\n    encoder_mask = torch.ones(\n        tot_num_obs_cells + bs * 2, tot_num_obs_cells + bs * 2, dtype=torch.bool\n    )\n\n    decoder_mask = torch.ones(bs * 2, bs * 2, dtype=torch.bool)\n    pooling_mask = torch.ones(bs, bs + tot_num_obs_cells, dtype=torch.bool)\n\n    offset = 0\n\n    for i, metadata in enumerate(batched_metadata):\n        num_hidden_cells = metadata.num_cells_obs\n        pooling_tok_idx = (2 * bs) - i\n        decoding_tok_idx = bs - i\n\n        indices: Int[np.ndarray, \"total_seqlen x_y\"] = (  # noqa: F722\n            index_outer_product(num_hidden_cells) + offset\n        )\n\n        pooling_indices: Int[np.ndarray, \"tot_seqlen\"] = np.full(  # noqa: F821\n            len(indices), -pooling_tok_idx\n        )\n\n        # if more than one decoding (ie not just single masked cell)\n        # need to form the product indices of (observed_cells, n_decoding_queries)\n        # and (cls_tokens, decoding_queries)\n        decoding_indices: Int[np.ndarray, \"tot_seqlen\"] = np.full(  # noqa: F821\n            len(indices), -decoding_tok_idx\n        )\n\n        x = indices[:, 0]\n        y = indices[:, 1]\n\n        encoder_mask[x, y] = False\n        encoder_mask[pooling_indices, y] = False\n        encoder_mask[x, pooling_indices] = False\n        encoder_mask[-pooling_tok_idx, -pooling_tok_idx] = False\n\n        encoder_mask[decoding_indices, y] = False\n        encoder_mask[x, decoding_indices] = False\n        encoder_mask[decoding_indices, decoding_indices] = False\n\n        encoder_mask[pooling_indices, decoding_indices] = False\n        encoder_mask[decoding_indices, pooling_indices] = False\n\n        pooling_mask[i, offset : offset + num_hidden_cells] = False\n\n        decoder_mask[i, bs + i] = False\n        decoder_mask[bs + i, i] = False\n\n        offset += num_hidden_cells\n\n        observed_expression.append(metadata.observed_expression)\n        masked_expression.append(metadata.masked_expression)\n        observed_cell_type.append(metadata.observed_cell_type)\n        masked_cell_type.append(metadata.masked_cell_type)\n        observed_neighboorhood_lens.append(metadata.num_cells_obs)\n\n    observed_expression = torch.cat(observed_expression).float()\n    masked_expression = torch.cat(masked_expression).float()\n    observed_cell_type = torch.cat(observed_cell_type).long()\n    masked_cell_type = torch.cat(masked_cell_type).long()\n\n    num_cells_plus_cls = sum(observed_neighboorhood_lens) + bs\n\n    return dict(\n        observed_expression=observed_expression,\n        masked_expression=masked_expression,\n        observed_cell_type=observed_cell_type,\n        masked_cell_type=masked_cell_type,\n        observed_neighboorhood_lens=observed_neighboorhood_lens,\n        full_mask=encoder_mask,\n        pooling_mask=pooling_mask,\n        encoder_mask=encoder_mask[:num_cells_plus_cls, :num_cells_plus_cls],\n        decoder_mask=decoder_mask,\n        bs=bs,\n    )\n</code></pre>"},{"location":"reference/data/_loader_polars/","title":"_loader_polars","text":""},{"location":"reference/data/attn_loader/","title":"attn_loader","text":"<p>TODO:         * need to:                 1. load all cells by genes                  2. two outputs: n_cells * genes long vec of integers for each gene and then                          separately the float valued directions                         maybe pretty low dim for this first embed, but who knows                 3. then need to aggregate via pooling or averaging, whatever                 4. how do we agg in this way?                 5. </p> <pre><code>            I figured that part out.\n\n            Now I need to find a way to extract for each cell:\n\n            (gene_ids, expr) where gene_ids is a list of integers and expr is the scalars\n\n            i do this at encoding time, now i need a fn to do it at decoding time\n</code></pre>"},{"location":"reference/data/loader_pandas/","title":"loader_pandas","text":""},{"location":"reference/data/loader_pandas/#data.loader_pandas.CenterMaskSampler","title":"<code>CenterMaskSampler</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>class CenterMaskSampler(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        metadata: pd.DataFrame,\n        adata: ad.AnnData,\n        patch_size: Union[List[int], Tuple[int]],\n        cell_id_colname: Optional[str] = \"cell_label\",\n        cell_type_colname: Optional[str] = \"cell_type\",\n        tissue_section_colname: Optional[str] = \"brain_section_label\",\n        max_num_cells: Optional[Union[int, None]] = None,\n        indices: Optional[Union[List[int], None]] = None,\n    ):\n        \"\"\"Sampler that returns the gene expression matrices and cell-type identity vectors\n         for two groups of cells: the observed cells and the masked/reference cells. The observed cells\n         are the cells in the neighborhood of the reference cell (set by `patch_size`).\n\n         We assume the type will be found at `cell_type_colname` and that `cell_id` can be used\n         succesfully to index into the adata object.\n\n        Parameters\n        ----------\n        metadata : pd.DataFrame\n            Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.\n        adata : ad.AnnData\n            Expression-containing (as .X) anndata. We assume input will be log scaled.\n        patch_size : Union[List[int], Tuple[int]]\n            Size in arbitrary units for the neighborhood calculation.\n        cell_id_colname : Optional[str], optional\n            The column to use to index into the anndata, by default \"cell_label\"\n        cell_type_colname : Optional[str], optional\n            The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"\n        tissue_section_colname : Optional[str], optional\n            To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"\n        max_num_cells : Optional[Union[int, None]], optional\n            How many cells to threshold at for the neighborhood size, by default None\n        indices: Optional[Union[List[int], None]], optional\n            Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with \n            `.iloc`, so it may be advisable to reset the index of the dataframe. By default None\n\n        Raises\n        ------\n        TypeError\n            If adata is not an `ad.annData` object\n        ValueError\n            If metadata does not contain the necessary columns\n        TypeError\n            If metadata is not a pandas DataFrame\n        ValueError\n            If patch_size is not a tuple of length 2\n        ValueError\n            If metadata and adata are not the same length\n        ValueError\n            If metadata does not contain columns \"x\" and \"y\"\n        \"\"\"\n        if isinstance(adata, ad.AnnData) is False:\n            raise TypeError(\n                f\"Input argument adata must be of type np.ndarray; got {type(adata)}.\"\n            )\n\n        if not all(\n            (\n                cell_type_colname in metadata.columns,\n                cell_id_colname in metadata.columns,\n                tissue_section_colname in metadata.columns,\n            )\n        ):\n            raise ValueError(\n                \"Provided metadata has to have columns with columns specified by cell_id_colname, cell_type_colname and tissue_section_colname.\"\n            )\n\n        if isinstance(metadata, pd.DataFrame) is False:\n            raise TypeError(\n                f\"Input argument metadata must be of type pl.DataFrame or pd.DataFrame; got {type(metadata)}.\"\n            )\n\n        if len(patch_size) != 2:\n            raise ValueError(\n                f\"Input argument patch_size must be a tuple of length 2 with desired dims order (x, y, z); got {len(patch_size)}.\"\n            )\n\n        if len(metadata) != len(adata):\n            raise ValueError(\"Metadata and adata must have the same length.\")\n\n        if not (\"x\" in metadata.columns and \"y\" in metadata.columns):\n            raise ValueError(\n                \"Metadata must contain columns named x and y to index the spatial coordinates of the cells.\"\n            )\n\n        self.metadata = metadata\n\n        # use these later to index into metadata dataframe\n        self.cell_type_colname = cell_type_colname\n        self.cell_id_colname = cell_id_colname\n        self.tissue_section_colname = tissue_section_colname\n        self.length = len(self.metadata) if indices is None else len(indices)\n        self.max_num_cells = np.inf if max_num_cells is None else max_num_cells\n\n        self.adata = adata\n        self.patch_size = np.array(patch_size).astype(int)\n        #self.noise_fac = 2\n\n        self._preprocess_metadata()\n\n        # so that later we can simply pass a train/valid/test set of indices\n        # instead of filtering up front\n        self.indices = indices if indices is not None else list(range(self.length))\n\n    def __len__(self):\n        return self.length\n\n    def _preprocess_metadata(self):\n        # for use in reducing complexity of neighborhood queries in getitem\n        self.tissue_section_mapper = {\n            section_label: subset_df\n            for section_label, subset_df in self.metadata.groupby(\n                self.tissue_section_colname\n            )\n        }\n\n    def get_nearby_cells(\n        self,\n        centroid: npt.ArrayLike,\n        patch_size: npt.ArrayLike,\n        tissue_section: pd.DataFrame,\n        discretize: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Return the cells from tissue_section that are within\n        patch_size distance away from centroid.\n\n        Parameters\n        ----------\n        centroid : npt.ArrayLike\n            A 1x2 vector representing the centroid of the patch.\n        patch_size : npt.ArrayLike\n            A 1x2 vector representing the size of the patch.\n        tissue_section : pd.DataFrame\n            A pandas DataFrame containing the cells in the tissue section.\n            It's assumed that the cell corresponding to centroid is in this dataframe.\n        discretize : bool, optional\n            _description_, by default False\n\n        Returns\n        -------\n        pd.DataFrame\n            Cells inside the neighborhood, inclusive of the\n            centroid cell / reference cell.\n        \"\"\"\n        start = centroid - (patch_size / 2)\n        if discretize:\n            start = np.rint(start).astype(int)\n\n        end = start + patch_size\n\n        lookup_x = tissue_section[\"x\"].between(start[0], end[0])\n        lookup_y = tissue_section[\"y\"].between(start[1], end[1])\n\n        return tissue_section[lookup_x &amp; lookup_y]\n\n    def idx_to_expr(self, indices: Union[str, int, List[Union[str, int]]]):\n        return self.adata[indices].X\n\n    def __getitem__(self, index) -&gt; dict:\n        orig_idx = self.indices[index]  # see init for expln\n        metadata_row = self.metadata.iloc[orig_idx]\n        index = metadata_row[self.cell_id_colname]\n\n        centroid = np.array([metadata_row[\"x\"], metadata_row[\"y\"]])\n#        centroid = add_gaussian(centroid, self.noise_fac)\n\n        tissue_section: pd.DataFrame = self.tissue_section_mapper.get(\n            metadata_row[self.tissue_section_colname], None\n        )\n\n        assert tissue_section is not None, (\n            \"Tissue section not found in `tissue_section_mapper` dict. \"\n            \"Check that the tissue_section_colname is correctly set in the metadata.\"\n        )\n\n        all_neighborhood_cells = self.get_nearby_cells(\n            centroid, self.patch_size, tissue_section, discretize=True\n        )\n\n        if len(all_neighborhood_cells) == 1:\n            # manually create an empty neighborhood\n            masked_expression: Float[np.array, \"one n_genes\"] = self.idx_to_expr(index)  # noqa: F722\n            # check if is csr, if so convert\n            if not isinstance(masked_expression, np.ndarray):\n                masked_expression = masked_expression.toarray()\n            masked_cell_type: int = metadata_row[self.cell_type_colname]\n\n            neighborhood_metadata = NeighborhoodMetadata(\n                observed_expression=torch.FloatTensor([]),\n                masked_expression=torch.from_numpy(masked_expression),\n                observed_cell_type=torch.LongTensor([]),\n                masked_cell_type=torch.LongTensor([masked_cell_type]),\n                num_cells_obs=0,\n            )\n            return neighborhood_metadata\n\n        return self.neighborhood_gather(all_neighborhood_cells, index, as_dict=False)\n\n    def neighborhood_gather(\n        self,\n        neighborhood_cells: pd.DataFrame,\n        ref_cell_label: str,\n        as_dict: Optional[bool] = False,\n    ):\n        \"\"\"Get the neighborhood cells' metadata and partition them into two sets.\n        In this case we will only predict the center / reference cell.\n\n        Parameters\n        ----------\n        neighborhood_cells : pd.DataFrame\n            Dataframe containing the cells in the neighborhood. One\n            cell's metadata per row.\n        ref_cell_label : str\n            The label of the reference cell as a string.\n        as_dict : Optional[bool], optional\n            Return a dictionary (True) or NeighborhoodMetadata (False),\n            by default False.\n\n        Returns\n        -------\n        Union[NeighborhoodMetadata, dict]\n            A namedtuple or dictionary containing the partitioned cells'\n            metadata, meaning the expression and class label integer for\n            each of the cells. The number of observed cells is also returned,\n            so in total there will be five entries / keys.\n\n        \"\"\"\n\n        neighborhood_cells = neighborhood_cells.reset_index(drop=True)\n        neighborhood_cells_indices = neighborhood_cells[self.cell_id_colname]\n        expression = self.adata[neighborhood_cells_indices].X\n\n        # check if is csr, if so convert\n        if not isinstance(expression, np.ndarray):\n            expression = expression.toarray()\n\n        num_cells_neighborhood = len(neighborhood_cells)\n\n        observed_cells: pd.DataFrame = neighborhood_cells[\n            neighborhood_cells_indices != ref_cell_label\n        ]\n\n        masked_cell = neighborhood_cells[neighborhood_cells_indices == ref_cell_label]\n\n        if self.max_num_cells &lt; (num_cells_neighborhood - 1):\n            random_indices: List[int] = random_indices_from_series(\n                observed_cells[self.cell_id_colname], self.max_num_cells\n            )\n            observed_cells = observed_cells.iloc[random_indices]\n\n        observed_cell_types = observed_cells[self.cell_type_colname].values\n        observed_expression: Float[np.ndarray, \"cells genes\"] = expression[observed_cells.index] # noqa: F722\n\n        masked_cell_types = masked_cell[self.cell_type_colname].values\n        masked_expression: Float[np.ndarray, \"cells genes\"] = expression[masked_cell.index] # noqa: F722\n\n        num_cells_obs = len(observed_cells)\n\n        if as_dict:\n            return dict(\n                observed_expression=observed_expression,\n                masked_expression=masked_expression,\n                observed_cell_type=observed_cell_types,\n                masked_cell_type=masked_cell_types,\n                num_cells_obs=num_cells_obs,\n            )\n        else:\n            return NeighborhoodMetadata(\n                torch.from_numpy(observed_expression),\n                torch.from_numpy(masked_expression),\n                torch.from_numpy(observed_cell_types),\n                torch.from_numpy(masked_cell_types),\n                num_cells_obs,\n            )\n</code></pre>"},{"location":"reference/data/loader_pandas/#data.loader_pandas.CenterMaskSampler.__init__","title":"<code>__init__(metadata, adata, patch_size, cell_id_colname='cell_label', cell_type_colname='cell_type', tissue_section_colname='brain_section_label', max_num_cells=None, indices=None)</code>","text":"<p>Sampler that returns the gene expression matrices and cell-type identity vectors  for two groups of cells: the observed cells and the masked/reference cells. The observed cells  are the cells in the neighborhood of the reference cell (set by <code>patch_size</code>).</p> <p>We assume the type will be found at <code>cell_type_colname</code> and that <code>cell_id</code> can be used  succesfully to index into the adata object.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.</p> required <code>adata</code> <code>AnnData</code> <p>Expression-containing (as .X) anndata. We assume input will be log scaled.</p> required <code>patch_size</code> <code>Union[List[int], Tuple[int]]</code> <p>Size in arbitrary units for the neighborhood calculation.</p> required <code>cell_id_colname</code> <code>Optional[str]</code> <p>The column to use to index into the anndata, by default \"cell_label\"</p> <code>'cell_label'</code> <code>cell_type_colname</code> <code>Optional[str]</code> <p>The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"</p> <code>'cell_type'</code> <code>tissue_section_colname</code> <code>Optional[str]</code> <p>To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"</p> <code>'brain_section_label'</code> <code>max_num_cells</code> <code>Optional[Union[int, None]]</code> <p>How many cells to threshold at for the neighborhood size, by default None</p> <code>None</code> <code>indices</code> <code>Optional[Union[List[int], None]]</code> <p>Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with  <code>.iloc</code>, so it may be advisable to reset the index of the dataframe. By default None</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If adata is not an <code>ad.annData</code> object</p> <code>ValueError</code> <p>If metadata does not contain the necessary columns</p> <code>TypeError</code> <p>If metadata is not a pandas DataFrame</p> <code>ValueError</code> <p>If patch_size is not a tuple of length 2</p> <code>ValueError</code> <p>If metadata and adata are not the same length</p> <code>ValueError</code> <p>If metadata does not contain columns \"x\" and \"y\"</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def __init__(\n    self,\n    metadata: pd.DataFrame,\n    adata: ad.AnnData,\n    patch_size: Union[List[int], Tuple[int]],\n    cell_id_colname: Optional[str] = \"cell_label\",\n    cell_type_colname: Optional[str] = \"cell_type\",\n    tissue_section_colname: Optional[str] = \"brain_section_label\",\n    max_num_cells: Optional[Union[int, None]] = None,\n    indices: Optional[Union[List[int], None]] = None,\n):\n    \"\"\"Sampler that returns the gene expression matrices and cell-type identity vectors\n     for two groups of cells: the observed cells and the masked/reference cells. The observed cells\n     are the cells in the neighborhood of the reference cell (set by `patch_size`).\n\n     We assume the type will be found at `cell_type_colname` and that `cell_id` can be used\n     succesfully to index into the adata object.\n\n    Parameters\n    ----------\n    metadata : pd.DataFrame\n        Metadata for the cells. Must contain columns for cell_id, cell_type, x, y, and tissue_section.\n    adata : ad.AnnData\n        Expression-containing (as .X) anndata. We assume input will be log scaled.\n    patch_size : Union[List[int], Tuple[int]]\n        Size in arbitrary units for the neighborhood calculation.\n    cell_id_colname : Optional[str], optional\n        The column to use to index into the anndata, by default \"cell_label\"\n    cell_type_colname : Optional[str], optional\n        The column to use to access the cell type identities as cls-encoding integers, by default \"cell_type\"\n    tissue_section_colname : Optional[str], optional\n        To simplify computation, group the cells in each sectionsby this column, by default \"brain_section_label\"\n    max_num_cells : Optional[Union[int, None]], optional\n        How many cells to threshold at for the neighborhood size, by default None\n    indices: Optional[Union[List[int], None]], optional\n        Used to specify train/test sets via subsetting on only these cells. This should be a numeric index compatible with \n        `.iloc`, so it may be advisable to reset the index of the dataframe. By default None\n\n    Raises\n    ------\n    TypeError\n        If adata is not an `ad.annData` object\n    ValueError\n        If metadata does not contain the necessary columns\n    TypeError\n        If metadata is not a pandas DataFrame\n    ValueError\n        If patch_size is not a tuple of length 2\n    ValueError\n        If metadata and adata are not the same length\n    ValueError\n        If metadata does not contain columns \"x\" and \"y\"\n    \"\"\"\n    if isinstance(adata, ad.AnnData) is False:\n        raise TypeError(\n            f\"Input argument adata must be of type np.ndarray; got {type(adata)}.\"\n        )\n\n    if not all(\n        (\n            cell_type_colname in metadata.columns,\n            cell_id_colname in metadata.columns,\n            tissue_section_colname in metadata.columns,\n        )\n    ):\n        raise ValueError(\n            \"Provided metadata has to have columns with columns specified by cell_id_colname, cell_type_colname and tissue_section_colname.\"\n        )\n\n    if isinstance(metadata, pd.DataFrame) is False:\n        raise TypeError(\n            f\"Input argument metadata must be of type pl.DataFrame or pd.DataFrame; got {type(metadata)}.\"\n        )\n\n    if len(patch_size) != 2:\n        raise ValueError(\n            f\"Input argument patch_size must be a tuple of length 2 with desired dims order (x, y, z); got {len(patch_size)}.\"\n        )\n\n    if len(metadata) != len(adata):\n        raise ValueError(\"Metadata and adata must have the same length.\")\n\n    if not (\"x\" in metadata.columns and \"y\" in metadata.columns):\n        raise ValueError(\n            \"Metadata must contain columns named x and y to index the spatial coordinates of the cells.\"\n        )\n\n    self.metadata = metadata\n\n    # use these later to index into metadata dataframe\n    self.cell_type_colname = cell_type_colname\n    self.cell_id_colname = cell_id_colname\n    self.tissue_section_colname = tissue_section_colname\n    self.length = len(self.metadata) if indices is None else len(indices)\n    self.max_num_cells = np.inf if max_num_cells is None else max_num_cells\n\n    self.adata = adata\n    self.patch_size = np.array(patch_size).astype(int)\n    #self.noise_fac = 2\n\n    self._preprocess_metadata()\n\n    # so that later we can simply pass a train/valid/test set of indices\n    # instead of filtering up front\n    self.indices = indices if indices is not None else list(range(self.length))\n</code></pre>"},{"location":"reference/data/loader_pandas/#data.loader_pandas.CenterMaskSampler.get_nearby_cells","title":"<code>get_nearby_cells(centroid, patch_size, tissue_section, discretize=False)</code>","text":"<p>Return the cells from tissue_section that are within patch_size distance away from centroid.</p> <p>Parameters:</p> Name Type Description Default <code>centroid</code> <code>ArrayLike</code> <p>A 1x2 vector representing the centroid of the patch.</p> required <code>patch_size</code> <code>ArrayLike</code> <p>A 1x2 vector representing the size of the patch.</p> required <code>tissue_section</code> <code>DataFrame</code> <p>A pandas DataFrame containing the cells in the tissue section. It's assumed that the cell corresponding to centroid is in this dataframe.</p> required <code>discretize</code> <code>bool</code> <p>description, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Cells inside the neighborhood, inclusive of the centroid cell / reference cell.</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def get_nearby_cells(\n    self,\n    centroid: npt.ArrayLike,\n    patch_size: npt.ArrayLike,\n    tissue_section: pd.DataFrame,\n    discretize: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Return the cells from tissue_section that are within\n    patch_size distance away from centroid.\n\n    Parameters\n    ----------\n    centroid : npt.ArrayLike\n        A 1x2 vector representing the centroid of the patch.\n    patch_size : npt.ArrayLike\n        A 1x2 vector representing the size of the patch.\n    tissue_section : pd.DataFrame\n        A pandas DataFrame containing the cells in the tissue section.\n        It's assumed that the cell corresponding to centroid is in this dataframe.\n    discretize : bool, optional\n        _description_, by default False\n\n    Returns\n    -------\n    pd.DataFrame\n        Cells inside the neighborhood, inclusive of the\n        centroid cell / reference cell.\n    \"\"\"\n    start = centroid - (patch_size / 2)\n    if discretize:\n        start = np.rint(start).astype(int)\n\n    end = start + patch_size\n\n    lookup_x = tissue_section[\"x\"].between(start[0], end[0])\n    lookup_y = tissue_section[\"y\"].between(start[1], end[1])\n\n    return tissue_section[lookup_x &amp; lookup_y]\n</code></pre>"},{"location":"reference/data/loader_pandas/#data.loader_pandas.CenterMaskSampler.neighborhood_gather","title":"<code>neighborhood_gather(neighborhood_cells, ref_cell_label, as_dict=False)</code>","text":"<p>Get the neighborhood cells' metadata and partition them into two sets. In this case we will only predict the center / reference cell.</p> <p>Parameters:</p> Name Type Description Default <code>neighborhood_cells</code> <code>DataFrame</code> <p>Dataframe containing the cells in the neighborhood. One cell's metadata per row.</p> required <code>ref_cell_label</code> <code>str</code> <p>The label of the reference cell as a string.</p> required <code>as_dict</code> <code>Optional[bool]</code> <p>Return a dictionary (True) or NeighborhoodMetadata (False), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[NeighborhoodMetadata, dict]</code> <p>A namedtuple or dictionary containing the partitioned cells' metadata, meaning the expression and class label integer for each of the cells. The number of observed cells is also returned, so in total there will be five entries / keys.</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def neighborhood_gather(\n    self,\n    neighborhood_cells: pd.DataFrame,\n    ref_cell_label: str,\n    as_dict: Optional[bool] = False,\n):\n    \"\"\"Get the neighborhood cells' metadata and partition them into two sets.\n    In this case we will only predict the center / reference cell.\n\n    Parameters\n    ----------\n    neighborhood_cells : pd.DataFrame\n        Dataframe containing the cells in the neighborhood. One\n        cell's metadata per row.\n    ref_cell_label : str\n        The label of the reference cell as a string.\n    as_dict : Optional[bool], optional\n        Return a dictionary (True) or NeighborhoodMetadata (False),\n        by default False.\n\n    Returns\n    -------\n    Union[NeighborhoodMetadata, dict]\n        A namedtuple or dictionary containing the partitioned cells'\n        metadata, meaning the expression and class label integer for\n        each of the cells. The number of observed cells is also returned,\n        so in total there will be five entries / keys.\n\n    \"\"\"\n\n    neighborhood_cells = neighborhood_cells.reset_index(drop=True)\n    neighborhood_cells_indices = neighborhood_cells[self.cell_id_colname]\n    expression = self.adata[neighborhood_cells_indices].X\n\n    # check if is csr, if so convert\n    if not isinstance(expression, np.ndarray):\n        expression = expression.toarray()\n\n    num_cells_neighborhood = len(neighborhood_cells)\n\n    observed_cells: pd.DataFrame = neighborhood_cells[\n        neighborhood_cells_indices != ref_cell_label\n    ]\n\n    masked_cell = neighborhood_cells[neighborhood_cells_indices == ref_cell_label]\n\n    if self.max_num_cells &lt; (num_cells_neighborhood - 1):\n        random_indices: List[int] = random_indices_from_series(\n            observed_cells[self.cell_id_colname], self.max_num_cells\n        )\n        observed_cells = observed_cells.iloc[random_indices]\n\n    observed_cell_types = observed_cells[self.cell_type_colname].values\n    observed_expression: Float[np.ndarray, \"cells genes\"] = expression[observed_cells.index] # noqa: F722\n\n    masked_cell_types = masked_cell[self.cell_type_colname].values\n    masked_expression: Float[np.ndarray, \"cells genes\"] = expression[masked_cell.index] # noqa: F722\n\n    num_cells_obs = len(observed_cells)\n\n    if as_dict:\n        return dict(\n            observed_expression=observed_expression,\n            masked_expression=masked_expression,\n            observed_cell_type=observed_cell_types,\n            masked_cell_type=masked_cell_types,\n            num_cells_obs=num_cells_obs,\n        )\n    else:\n        return NeighborhoodMetadata(\n            torch.from_numpy(observed_expression),\n            torch.from_numpy(masked_expression),\n            torch.from_numpy(observed_cell_types),\n            torch.from_numpy(masked_cell_types),\n            num_cells_obs,\n        )\n</code></pre>"},{"location":"reference/data/loader_pandas/#data.loader_pandas.collate","title":"<code>collate(batched_metadata)</code>","text":"<p>Collates metadata which is a list of NeighborhoodMetadata namedtuples. One important computation is the attention matrices that the transformer will use. In particular the <code>encoder_mask</code> which is a square matrix with (n_obs_cells + n_cls_tokens [=bs]) length. The <code>pooling_mask</code> is a matrix of shape (n_pooling_tokens [=bs], n_obs_cells + n_cls_tokens) which is used to pool the hidden states. The <code>decoder_mask</code> is a square matrix of shape (n_pooled_tokens [=bs], n_query_tokens [=bs]) which is used to mask the decoding queries. The other items (the expression matrices and integer-encoded cell type vector) are simply concatenated.</p> <p>Parameters:</p> Name Type Description Default <code>batched_metadata</code> <code>NeighborhoodMetadata</code> <p>Contains attributes: observed_expression, masked_expression, observed_cell_type, masked_cell_type, num_cells_obs Expression are float valued (n_cells x genes) matrices and cell types are integer-valued vectors.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the collated metadata and the attention masks.</p> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def collate(batched_metadata: NeighborhoodMetadata) -&gt; Dict[str, torch.Tensor | int]:\n    \"\"\"Collates metadata which is a list of NeighborhoodMetadata namedtuples.\n    One important computation is the attention matrices that the transformer will use.\n    In particular the `encoder_mask` which is a square matrix with (n_obs_cells + n_cls_tokens [=bs]) length.\n    The `pooling_mask` is a matrix of shape (n_pooling_tokens [=bs], n_obs_cells + n_cls_tokens) which is used to pool the hidden states.\n    The `decoder_mask` is a square matrix of shape (n_pooled_tokens [=bs], n_query_tokens [=bs]) which is used to mask the decoding queries.\n    The other items (the expression matrices and integer-encoded cell type vector) are simply concatenated.\n\n    Parameters\n    ----------\n    batched_metadata : NeighborhoodMetadata\n        Contains attributes: observed_expression, masked_expression, observed_cell_type, masked_cell_type, num_cells_obs\n        Expression are float valued (n_cells x genes) matrices and cell types are integer-valued vectors.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the collated metadata and the attention masks.\n    \"\"\"\n    observed_expression = []\n    masked_expression = []\n    observed_cell_type = []\n    masked_cell_type = []\n    observed_neighboorhood_lens = []\n\n    tot_num_obs_cells = sum([metadata.num_cells_obs for metadata in batched_metadata])\n    bs = len(batched_metadata)\n\n    encoder_mask = torch.ones(\n        tot_num_obs_cells + bs * 2, tot_num_obs_cells + bs * 2, dtype=torch.bool\n    )\n\n    decoder_mask = torch.ones(bs * 2, bs * 2, dtype=torch.bool)\n    pooling_mask = torch.ones(bs, bs + tot_num_obs_cells, dtype=torch.bool)\n\n    offset = 0\n\n    for i, metadata in enumerate(batched_metadata):\n        num_hidden_cells = metadata.num_cells_obs\n        pooling_tok_idx = (2 * bs) - i\n        decoding_tok_idx = bs - i\n\n        indices: Int[np.ndarray, \"total_seqlen x_y\"] = (  # noqa: F722\n            index_outer_product(num_hidden_cells) + offset\n        )\n\n        pooling_indices: Int[np.ndarray, \"tot_seqlen\"] = np.full(  # noqa: F821\n            len(indices), -pooling_tok_idx\n        )\n\n        # if more than one decoding (ie not just single masked cell)\n        # need to form the product indices of (observed_cells, n_decoding_queries)\n        # and (cls_tokens, decoding_queries)\n        decoding_indices: Int[np.ndarray, \"tot_seqlen\"] = np.full(  # noqa: F821\n            len(indices), -decoding_tok_idx\n        )\n\n        x = indices[:, 0]\n        y = indices[:, 1]\n\n        encoder_mask[x, y] = False\n        encoder_mask[pooling_indices, y] = False\n        encoder_mask[x, pooling_indices] = False\n        encoder_mask[-pooling_tok_idx, -pooling_tok_idx] = False\n\n        encoder_mask[decoding_indices, y] = False\n        encoder_mask[x, decoding_indices] = False\n        encoder_mask[decoding_indices, decoding_indices] = False\n\n        encoder_mask[pooling_indices, decoding_indices] = False\n        encoder_mask[decoding_indices, pooling_indices] = False\n\n        pooling_mask[i, offset : offset + num_hidden_cells] = False\n\n        decoder_mask[i, bs + i] = False\n        decoder_mask[bs + i, i] = False\n\n        offset += num_hidden_cells\n\n        observed_expression.append(metadata.observed_expression)\n        masked_expression.append(metadata.masked_expression)\n        observed_cell_type.append(metadata.observed_cell_type)\n        masked_cell_type.append(metadata.masked_cell_type)\n        observed_neighboorhood_lens.append(metadata.num_cells_obs)\n\n    observed_expression = torch.cat(observed_expression).float()\n    masked_expression = torch.cat(masked_expression).float()\n    observed_cell_type = torch.cat(observed_cell_type).long()\n    masked_cell_type = torch.cat(masked_cell_type).long()\n\n    num_cells_plus_cls = sum(observed_neighboorhood_lens) + bs\n\n    return dict(\n        observed_expression=observed_expression,\n        masked_expression=masked_expression,\n        observed_cell_type=observed_cell_type,\n        masked_cell_type=masked_cell_type,\n        observed_neighboorhood_lens=observed_neighboorhood_lens,\n        full_mask=encoder_mask,\n        pooling_mask=pooling_mask,\n        encoder_mask=encoder_mask[:num_cells_plus_cls, :num_cells_plus_cls],\n        decoder_mask=decoder_mask,\n        bs=bs,\n    )\n</code></pre>"},{"location":"reference/data/loader_pandas/#data.loader_pandas.index_outer_product","title":"<code>index_outer_product(n)</code>","text":"<p>Compute the product of the set of indices with itself and reshapes to (, 2). In other words, return all pairs of indices from the set (0, 1, ..., n).</p> <p>Returns:</p> Type Description <code>array of int</code> <p>(n_indices, 2) np array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(index_outer_product(3))\narray([[0, 0],\n    [1, 0],\n    [2, 0],\n    [0, 1],\n    [1, 1],\n    [2, 1],\n    [0, 2],\n    [1, 2],\n    [2, 2]])\n</code></pre> Source code in <code>brainformr/data/loader_pandas.py</code> <pre><code>def index_outer_product(n: int) -&gt; Int[np.ndarray, \"seqlen 2\"]:  # noqa: F722\n    \"\"\"Compute the product of the set of indices with itself and reshapes to (, 2). In other words,\n    return all pairs of indices from the set (0, 1, ..., n).\n\n    Returns\n    -------\n    array of int\n        (n_indices, 2) np array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; print(index_outer_product(3))\n    array([[0, 0],\n        [1, 0],\n        [2, 0],\n        [0, 1],\n        [1, 1],\n        [2, 1],\n        [0, 2],\n        [1, 2],\n        [2, 2]])\n    \"\"\"\n    i = np.arange(n)\n    grid = np.dstack(np.meshgrid(i, i)).reshape(-1, 2)\n    return grid\n</code></pre>"},{"location":"reference/model/","title":"model","text":""},{"location":"reference/model/#model.AttnPool","title":"<code>AttnPool</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>class AttnPool(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: Optional[float] = 0.0,\n        bias: Optional[bool] = False,\n        zero_attn: Optional[bool] = False,\n        norm: Optional[nn.Module] = nn.LayerNorm,\n    ):\n        \"\"\"A simple attention pool.\n\n        Args:\n            embed_dim (int): embedding dim\n            num_heads (int): number heads\n            dropout (Optional[float], optional): Dropout. Defaults to 0.0.\n            bias (Optional[bool], optional): Whether to use bias or not in MLP layers. Defaults to False.\n            zero_attn (Optional[bool], optional): Passthrough to MHA. Defaults to False.\n            norm (Optional[nn.Module], optional): Passthrough to MHA. Defaults to nn.LayerNorm.\n        \"\"\"\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(\n            embed_dim,\n            bias=bias,\n            dropout=dropout,\n            num_heads=num_heads,\n            add_bias_kv=bias,\n            add_zero_attn=zero_attn,\n        )\n\n        self.norm = norm(embed_dim)\n        self.norm_v = norm(embed_dim)\n        self.norm_k = norm(embed_dim)\n        self.norm2 = norm(embed_dim)\n        self.query_token = nn.Parameter(torch.zeros(1, embed_dim))\n        nn.init.normal_(self.query_token, std=0.02)\n\n        self.dropout_sa = nn.Dropout(dropout)\n\n    def forward(self, x, attn_mask, bs):\n        # query_token = query_token.expand(bs, -1)\n        query_token = self.norm(self.query_token.expand(bs, -1))\n\n        attn_output, attn_output_weights = self.attn(\n            query_token, self.norm_v(x), self.norm_k(x), attn_mask=attn_mask\n        )\n        attn_output = self.dropout_sa(attn_output)\n\n        return self.norm2(attn_output)\n</code></pre>"},{"location":"reference/model/#model.AttnPool.__init__","title":"<code>__init__(embed_dim, num_heads, dropout=0.0, bias=False, zero_attn=False, norm=nn.LayerNorm)</code>","text":"<p>A simple attention pool.</p> <p>Args:     embed_dim (int): embedding dim     num_heads (int): number heads     dropout (Optional[float], optional): Dropout. Defaults to 0.0.     bias (Optional[bool], optional): Whether to use bias or not in MLP layers. Defaults to False.     zero_attn (Optional[bool], optional): Passthrough to MHA. Defaults to False.     norm (Optional[nn.Module], optional): Passthrough to MHA. Defaults to nn.LayerNorm.</p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    dropout: Optional[float] = 0.0,\n    bias: Optional[bool] = False,\n    zero_attn: Optional[bool] = False,\n    norm: Optional[nn.Module] = nn.LayerNorm,\n):\n    \"\"\"A simple attention pool.\n\n    Args:\n        embed_dim (int): embedding dim\n        num_heads (int): number heads\n        dropout (Optional[float], optional): Dropout. Defaults to 0.0.\n        bias (Optional[bool], optional): Whether to use bias or not in MLP layers. Defaults to False.\n        zero_attn (Optional[bool], optional): Passthrough to MHA. Defaults to False.\n        norm (Optional[nn.Module], optional): Passthrough to MHA. Defaults to nn.LayerNorm.\n    \"\"\"\n    super().__init__()\n\n    self.attn = nn.MultiheadAttention(\n        embed_dim,\n        bias=bias,\n        dropout=dropout,\n        num_heads=num_heads,\n        add_bias_kv=bias,\n        add_zero_attn=zero_attn,\n    )\n\n    self.norm = norm(embed_dim)\n    self.norm_v = norm(embed_dim)\n    self.norm_k = norm(embed_dim)\n    self.norm2 = norm(embed_dim)\n    self.query_token = nn.Parameter(torch.zeros(1, embed_dim))\n    nn.init.normal_(self.query_token, std=0.02)\n\n    self.dropout_sa = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/model/#model.CellTransformer","title":"<code>CellTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/base.py</code> <pre><code>class CellTransformer(nn.Module):\n    def __init__(\n        self,\n        encoder_embedding_dim: int,\n        encoder_num_heads: int,\n        encoder_depth: int,\n        decoder_embedding_dim: int,\n        decoder_num_heads: int,\n        decoder_depth: int,\n        attn_pool_heads: Optional[int] = 8,\n        cell_cardinality: Optional[int] = 1024,\n        put_device: Optional[str] = \"cuda\",\n        eps: float = 1e-15,\n        n_genes: Optional[int] = 500,\n        xformer_dropout: Optional[float] = 0.0,\n        bias: Optional[bool] = False,\n        zero_attn: Optional[bool] = True,\n    ):\n        \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n        Parameters\n        ----------\n        encoder_embedding_dim : int\n        encoder_num_heads : int\n        encoder_depth : int\n        decoder_embedding_dim : int\n        decoder_num_heads : int\n        decoder_depth : int\n        attn_pool_heads : Optional[int], optional\n            Number attention pool heads, by default 8\n        cell_cardinality : Optional[int], optional\n            Cardinality/number of different cell types for embedding, by default 1024\n        put_device : Optional[str], optional\n            For later referencing where to put input tensors, by default 'cuda'\n        eps : float, optional\n            Stability additional constant for NB params, by default 1e-15\n        n_genes : Optional[int], optional\n            Number genes, by default 500\n        xformer_dropout : Optional[float], optional\n            Dropout %, by default 0.0\n        bias : Optional[bool], optional\n            Use bias or not, by default True\n        zero_attn : Optional[bool], optional\n            Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n        \"\"\"\n        super().__init__()\n\n        self.eps = eps\n\n        _feature_dim = encoder_embedding_dim // 2\n\n        self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n        self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n        self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n        self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n        self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n        self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n        self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                                  bias=True, zero_attn=True)\n        # no bias causes instability in training\n\n        self.encoder = set_up_transformer_layers(\n            encoder_embedding_dim,\n            encoder_num_heads,\n            encoder_depth,\n            xformer_dropout,\n            bias,\n            zero_attn,\n        )\n        #self.encoder = torch.compile(self.encoder)\n\n        self.decoder = set_up_transformer_layers(\n            decoder_embedding_dim,\n            decoder_num_heads,\n            decoder_depth,\n            xformer_dropout,\n            bias,\n            zero_attn,\n        )\n        #self.decoder = torch.compile(self.decoder)\n\n        self.zinb_proj = ZINBProj(\n            embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n        )\n\n        nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n        self.put_device = put_device\n\n    def forward(self, data_dict: dict):\n        bs = data_dict[\"bs\"]\n\n        cells = data_dict[\"observed_cell_type\"].to(self.put_device, non_blocking=False)\n        expression = data_dict[\"observed_expression\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        num_hidden = len(\n            data_dict[\"masked_cell_type\"]\n        )  # will in practice be == bs but in case later want to train on multiple cells\n        hidden_expression = data_dict[\"masked_expression\"].to(\n            self.put_device, non_blocking=False, dtype=torch.float32\n        )\n        hidden_cells = data_dict[\"masked_cell_type\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        pooling_mask = data_dict[\"pooling_mask\"].to(self.put_device, non_blocking=False)\n        decoder_mask = data_dict[\"decoder_mask\"].to(self.put_device, non_blocking=False)\n        encoder_mask = data_dict[\"encoder_mask\"].to(self.put_device, non_blocking=False)\n\n        cls_tokens = self.cls_token.repeat_interleave(bs, dim=0)\n\n        cells_embed = self.encoder_cell_embed(cells)\n\n        expression_embed = self.expression_projection(expression)\n        cells_embed = torch.cat((cells_embed, expression_embed), dim=1)\n        cells_embed = torch.cat((cells_embed, cls_tokens), dim=0)\n\n        cells_embed = self.proj_norm(cells_embed)\n\n        cells_embed = self.encoder(cells_embed, mask=encoder_mask)\n\n        attn_pool = self.attn_pool(cells_embed, pooling_mask, bs)\n\n        decoding_queries = self.decoder_cell_embed(hidden_cells)\n\n        cells_embed = torch.cat((attn_pool, decoding_queries), dim=0)\n\n        cells_embed = self.decoder(cells_embed, mask=decoder_mask)\n        ref_cell_embed = cells_embed[-bs:]\n\n        with torch.autocast(dtype=torch.float32, device_type=self.put_device):\n            # sometimes bfloat doesn't work here for lgamma backward (in zinb, otherwise will error)\n            zinb_params = self.zinb_proj(ref_cell_embed)\n\n        cls_toks = cells_embed[-(num_hidden + bs) : -num_hidden]\n\n        return dict(\n            zinb_params=zinb_params,\n            neighborhood_repr=cls_toks,\n            hidden_expression=hidden_expression,\n        )\n</code></pre>"},{"location":"reference/model/#model.CellTransformer.__init__","title":"<code>__init__(encoder_embedding_dim, encoder_num_heads, encoder_depth, decoder_embedding_dim, decoder_num_heads, decoder_depth, attn_pool_heads=8, cell_cardinality=1024, put_device='cuda', eps=1e-15, n_genes=500, xformer_dropout=0.0, bias=False, zero_attn=True)</code>","text":"<p>An encoder-decoder model with attention pooling prior to decoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_embedding_dim</code> <code>int</code> required <code>encoder_num_heads</code> <code>int</code> required <code>encoder_depth</code> <code>int</code> required <code>decoder_embedding_dim</code> <code>int</code> required <code>decoder_num_heads</code> <code>int</code> required <code>decoder_depth</code> <code>int</code> required <code>attn_pool_heads</code> <code>Optional[int]</code> <p>Number attention pool heads, by default 8</p> <code>8</code> <code>cell_cardinality</code> <code>Optional[int]</code> <p>Cardinality/number of different cell types for embedding, by default 1024</p> <code>1024</code> <code>put_device</code> <code>Optional[str]</code> <p>For later referencing where to put input tensors, by default 'cuda'</p> <code>'cuda'</code> <code>eps</code> <code>float</code> <p>Stability additional constant for NB params, by default 1e-15</p> <code>1e-15</code> <code>n_genes</code> <code>Optional[int]</code> <p>Number genes, by default 500</p> <code>500</code> <code>xformer_dropout</code> <code>Optional[float]</code> <p>Dropout %, by default 0.0</p> <code>0.0</code> <code>bias</code> <code>Optional[bool]</code> <p>Use bias or not, by default True</p> <code>False</code> <code>zero_attn</code> <code>Optional[bool]</code> <p>Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True</p> <code>True</code> Source code in <code>brainformr/model/base.py</code> <pre><code>def __init__(\n    self,\n    encoder_embedding_dim: int,\n    encoder_num_heads: int,\n    encoder_depth: int,\n    decoder_embedding_dim: int,\n    decoder_num_heads: int,\n    decoder_depth: int,\n    attn_pool_heads: Optional[int] = 8,\n    cell_cardinality: Optional[int] = 1024,\n    put_device: Optional[str] = \"cuda\",\n    eps: float = 1e-15,\n    n_genes: Optional[int] = 500,\n    xformer_dropout: Optional[float] = 0.0,\n    bias: Optional[bool] = False,\n    zero_attn: Optional[bool] = True,\n):\n    \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n    Parameters\n    ----------\n    encoder_embedding_dim : int\n    encoder_num_heads : int\n    encoder_depth : int\n    decoder_embedding_dim : int\n    decoder_num_heads : int\n    decoder_depth : int\n    attn_pool_heads : Optional[int], optional\n        Number attention pool heads, by default 8\n    cell_cardinality : Optional[int], optional\n        Cardinality/number of different cell types for embedding, by default 1024\n    put_device : Optional[str], optional\n        For later referencing where to put input tensors, by default 'cuda'\n    eps : float, optional\n        Stability additional constant for NB params, by default 1e-15\n    n_genes : Optional[int], optional\n        Number genes, by default 500\n    xformer_dropout : Optional[float], optional\n        Dropout %, by default 0.0\n    bias : Optional[bool], optional\n        Use bias or not, by default True\n    zero_attn : Optional[bool], optional\n        Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n    \"\"\"\n    super().__init__()\n\n    self.eps = eps\n\n    _feature_dim = encoder_embedding_dim // 2\n\n    self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n    self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n    self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n    self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n    self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n    self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n    self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                              bias=True, zero_attn=True)\n    # no bias causes instability in training\n\n    self.encoder = set_up_transformer_layers(\n        encoder_embedding_dim,\n        encoder_num_heads,\n        encoder_depth,\n        xformer_dropout,\n        bias,\n        zero_attn,\n    )\n    #self.encoder = torch.compile(self.encoder)\n\n    self.decoder = set_up_transformer_layers(\n        decoder_embedding_dim,\n        decoder_num_heads,\n        decoder_depth,\n        xformer_dropout,\n        bias,\n        zero_attn,\n    )\n    #self.decoder = torch.compile(self.decoder)\n\n    self.zinb_proj = ZINBProj(\n        embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n    )\n\n    nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n    self.put_device = put_device\n</code></pre>"},{"location":"reference/model/#model.ZINBProj","title":"<code>ZINBProj</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>class ZINBProj(nn.Module):\n\n    def __init__(self, embed_dim: int, n_genes: int, eps: float):\n        \"\"\"A utility block to regress the parameters of a (zero-inflated)\n        negative binomial distribution from an embedding. \n\n        Args:\n            embed_dim (int): embedding dim\n            n_genes (int): number of output genes to regress\n            eps (float): error floor for mu, theta, scale params\n        \"\"\"\n        super().__init__()\n        self.mu = nn.Linear(embed_dim, n_genes)  # mean\n        self.theta = nn.Linear(\n            embed_dim, n_genes\n        )  # inv. dispersion\n        self.scale = nn.Linear(\n            embed_dim, n_genes\n        )  # avg. expr.\n        self.gate_logit = nn.Linear(\n            embed_dim, n_genes\n        )  # zi\n\n        self.eps = eps\n\n    def forward(self, x) -&gt; Dict[str, Float[torch.Tensor, \"n_cells n_genes\"]]:\n        mu = self.mu(x).exp() + self.eps\n        theta = self.theta(x).exp() + self.eps\n        gate = self.gate_logit(x) \n        scale = self.scale(x).exp() + self.eps\n        return dict(mu=mu, theta=theta, zi_logits=gate, scale=scale)\n</code></pre>"},{"location":"reference/model/#model.ZINBProj.__init__","title":"<code>__init__(embed_dim, n_genes, eps)</code>","text":"<p>A utility block to regress the parameters of a (zero-inflated) negative binomial distribution from an embedding. </p> <p>Args:     embed_dim (int): embedding dim     n_genes (int): number of output genes to regress     eps (float): error floor for mu, theta, scale params</p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>def __init__(self, embed_dim: int, n_genes: int, eps: float):\n    \"\"\"A utility block to regress the parameters of a (zero-inflated)\n    negative binomial distribution from an embedding. \n\n    Args:\n        embed_dim (int): embedding dim\n        n_genes (int): number of output genes to regress\n        eps (float): error floor for mu, theta, scale params\n    \"\"\"\n    super().__init__()\n    self.mu = nn.Linear(embed_dim, n_genes)  # mean\n    self.theta = nn.Linear(\n        embed_dim, n_genes\n    )  # inv. dispersion\n    self.scale = nn.Linear(\n        embed_dim, n_genes\n    )  # avg. expr.\n    self.gate_logit = nn.Linear(\n        embed_dim, n_genes\n    )  # zi\n\n    self.eps = eps\n</code></pre>"},{"location":"reference/model/base/","title":"base","text":""},{"location":"reference/model/base/#model.base.CellTransformer","title":"<code>CellTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/base.py</code> <pre><code>class CellTransformer(nn.Module):\n    def __init__(\n        self,\n        encoder_embedding_dim: int,\n        encoder_num_heads: int,\n        encoder_depth: int,\n        decoder_embedding_dim: int,\n        decoder_num_heads: int,\n        decoder_depth: int,\n        attn_pool_heads: Optional[int] = 8,\n        cell_cardinality: Optional[int] = 1024,\n        put_device: Optional[str] = \"cuda\",\n        eps: float = 1e-15,\n        n_genes: Optional[int] = 500,\n        xformer_dropout: Optional[float] = 0.0,\n        bias: Optional[bool] = False,\n        zero_attn: Optional[bool] = True,\n    ):\n        \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n        Parameters\n        ----------\n        encoder_embedding_dim : int\n        encoder_num_heads : int\n        encoder_depth : int\n        decoder_embedding_dim : int\n        decoder_num_heads : int\n        decoder_depth : int\n        attn_pool_heads : Optional[int], optional\n            Number attention pool heads, by default 8\n        cell_cardinality : Optional[int], optional\n            Cardinality/number of different cell types for embedding, by default 1024\n        put_device : Optional[str], optional\n            For later referencing where to put input tensors, by default 'cuda'\n        eps : float, optional\n            Stability additional constant for NB params, by default 1e-15\n        n_genes : Optional[int], optional\n            Number genes, by default 500\n        xformer_dropout : Optional[float], optional\n            Dropout %, by default 0.0\n        bias : Optional[bool], optional\n            Use bias or not, by default True\n        zero_attn : Optional[bool], optional\n            Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n        \"\"\"\n        super().__init__()\n\n        self.eps = eps\n\n        _feature_dim = encoder_embedding_dim // 2\n\n        self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n        self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n        self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n        self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n        self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n        self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n        self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                                  bias=True, zero_attn=True)\n        # no bias causes instability in training\n\n        self.encoder = set_up_transformer_layers(\n            encoder_embedding_dim,\n            encoder_num_heads,\n            encoder_depth,\n            xformer_dropout,\n            bias,\n            zero_attn,\n        )\n        #self.encoder = torch.compile(self.encoder)\n\n        self.decoder = set_up_transformer_layers(\n            decoder_embedding_dim,\n            decoder_num_heads,\n            decoder_depth,\n            xformer_dropout,\n            bias,\n            zero_attn,\n        )\n        #self.decoder = torch.compile(self.decoder)\n\n        self.zinb_proj = ZINBProj(\n            embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n        )\n\n        nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n        self.put_device = put_device\n\n    def forward(self, data_dict: dict):\n        bs = data_dict[\"bs\"]\n\n        cells = data_dict[\"observed_cell_type\"].to(self.put_device, non_blocking=False)\n        expression = data_dict[\"observed_expression\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        num_hidden = len(\n            data_dict[\"masked_cell_type\"]\n        )  # will in practice be == bs but in case later want to train on multiple cells\n        hidden_expression = data_dict[\"masked_expression\"].to(\n            self.put_device, non_blocking=False, dtype=torch.float32\n        )\n        hidden_cells = data_dict[\"masked_cell_type\"].to(\n            self.put_device, non_blocking=False\n        )\n\n        pooling_mask = data_dict[\"pooling_mask\"].to(self.put_device, non_blocking=False)\n        decoder_mask = data_dict[\"decoder_mask\"].to(self.put_device, non_blocking=False)\n        encoder_mask = data_dict[\"encoder_mask\"].to(self.put_device, non_blocking=False)\n\n        cls_tokens = self.cls_token.repeat_interleave(bs, dim=0)\n\n        cells_embed = self.encoder_cell_embed(cells)\n\n        expression_embed = self.expression_projection(expression)\n        cells_embed = torch.cat((cells_embed, expression_embed), dim=1)\n        cells_embed = torch.cat((cells_embed, cls_tokens), dim=0)\n\n        cells_embed = self.proj_norm(cells_embed)\n\n        cells_embed = self.encoder(cells_embed, mask=encoder_mask)\n\n        attn_pool = self.attn_pool(cells_embed, pooling_mask, bs)\n\n        decoding_queries = self.decoder_cell_embed(hidden_cells)\n\n        cells_embed = torch.cat((attn_pool, decoding_queries), dim=0)\n\n        cells_embed = self.decoder(cells_embed, mask=decoder_mask)\n        ref_cell_embed = cells_embed[-bs:]\n\n        with torch.autocast(dtype=torch.float32, device_type=self.put_device):\n            # sometimes bfloat doesn't work here for lgamma backward (in zinb, otherwise will error)\n            zinb_params = self.zinb_proj(ref_cell_embed)\n\n        cls_toks = cells_embed[-(num_hidden + bs) : -num_hidden]\n\n        return dict(\n            zinb_params=zinb_params,\n            neighborhood_repr=cls_toks,\n            hidden_expression=hidden_expression,\n        )\n</code></pre>"},{"location":"reference/model/base/#model.base.CellTransformer.__init__","title":"<code>__init__(encoder_embedding_dim, encoder_num_heads, encoder_depth, decoder_embedding_dim, decoder_num_heads, decoder_depth, attn_pool_heads=8, cell_cardinality=1024, put_device='cuda', eps=1e-15, n_genes=500, xformer_dropout=0.0, bias=False, zero_attn=True)</code>","text":"<p>An encoder-decoder model with attention pooling prior to decoder.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_embedding_dim</code> <code>int</code> required <code>encoder_num_heads</code> <code>int</code> required <code>encoder_depth</code> <code>int</code> required <code>decoder_embedding_dim</code> <code>int</code> required <code>decoder_num_heads</code> <code>int</code> required <code>decoder_depth</code> <code>int</code> required <code>attn_pool_heads</code> <code>Optional[int]</code> <p>Number attention pool heads, by default 8</p> <code>8</code> <code>cell_cardinality</code> <code>Optional[int]</code> <p>Cardinality/number of different cell types for embedding, by default 1024</p> <code>1024</code> <code>put_device</code> <code>Optional[str]</code> <p>For later referencing where to put input tensors, by default 'cuda'</p> <code>'cuda'</code> <code>eps</code> <code>float</code> <p>Stability additional constant for NB params, by default 1e-15</p> <code>1e-15</code> <code>n_genes</code> <code>Optional[int]</code> <p>Number genes, by default 500</p> <code>500</code> <code>xformer_dropout</code> <code>Optional[float]</code> <p>Dropout %, by default 0.0</p> <code>0.0</code> <code>bias</code> <code>Optional[bool]</code> <p>Use bias or not, by default True</p> <code>False</code> <code>zero_attn</code> <code>Optional[bool]</code> <p>Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True</p> <code>True</code> Source code in <code>brainformr/model/base.py</code> <pre><code>def __init__(\n    self,\n    encoder_embedding_dim: int,\n    encoder_num_heads: int,\n    encoder_depth: int,\n    decoder_embedding_dim: int,\n    decoder_num_heads: int,\n    decoder_depth: int,\n    attn_pool_heads: Optional[int] = 8,\n    cell_cardinality: Optional[int] = 1024,\n    put_device: Optional[str] = \"cuda\",\n    eps: float = 1e-15,\n    n_genes: Optional[int] = 500,\n    xformer_dropout: Optional[float] = 0.0,\n    bias: Optional[bool] = False,\n    zero_attn: Optional[bool] = True,\n):\n    \"\"\"An encoder-decoder model with attention pooling prior to decoder.\n\n    Parameters\n    ----------\n    encoder_embedding_dim : int\n    encoder_num_heads : int\n    encoder_depth : int\n    decoder_embedding_dim : int\n    decoder_num_heads : int\n    decoder_depth : int\n    attn_pool_heads : Optional[int], optional\n        Number attention pool heads, by default 8\n    cell_cardinality : Optional[int], optional\n        Cardinality/number of different cell types for embedding, by default 1024\n    put_device : Optional[str], optional\n        For later referencing where to put input tensors, by default 'cuda'\n    eps : float, optional\n        Stability additional constant for NB params, by default 1e-15\n    n_genes : Optional[int], optional\n        Number genes, by default 500\n    xformer_dropout : Optional[float], optional\n        Dropout %, by default 0.0\n    bias : Optional[bool], optional\n        Use bias or not, by default True\n    zero_attn : Optional[bool], optional\n        Enable zero attention (zeros appended to k/v seqs to allow for \"attending to nothing\"), by default True\n    \"\"\"\n    super().__init__()\n\n    self.eps = eps\n\n    _feature_dim = encoder_embedding_dim // 2\n\n    self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n    self.expression_projection = get_projection_layers(n_genes, _feature_dim)\n    self.encoder_cell_embed = nn.Embedding(cell_cardinality, _feature_dim)\n\n    self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n    self.proj_norm = nn.LayerNorm(encoder_embedding_dim)\n\n    self.decoder_cell_embed = nn.Embedding(cell_cardinality, decoder_embedding_dim)\n    self.attn_pool = AttnPool(encoder_embedding_dim, attn_pool_heads,\n                              bias=True, zero_attn=True)\n    # no bias causes instability in training\n\n    self.encoder = set_up_transformer_layers(\n        encoder_embedding_dim,\n        encoder_num_heads,\n        encoder_depth,\n        xformer_dropout,\n        bias,\n        zero_attn,\n    )\n    #self.encoder = torch.compile(self.encoder)\n\n    self.decoder = set_up_transformer_layers(\n        decoder_embedding_dim,\n        decoder_num_heads,\n        decoder_depth,\n        xformer_dropout,\n        bias,\n        zero_attn,\n    )\n    #self.decoder = torch.compile(self.decoder)\n\n    self.zinb_proj = ZINBProj(\n        embed_dim=decoder_embedding_dim, n_genes=n_genes, eps=self.eps\n    )\n\n    nn.init.normal_(self.cls_token, std=0.1) #std=0.02)\n\n    self.put_device = put_device\n</code></pre>"},{"location":"reference/model/blocks/","title":"blocks","text":""},{"location":"reference/model/blocks/#model.blocks.AttnPool","title":"<code>AttnPool</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>class AttnPool(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: Optional[float] = 0.0,\n        bias: Optional[bool] = False,\n        zero_attn: Optional[bool] = False,\n        norm: Optional[nn.Module] = nn.LayerNorm,\n    ):\n        \"\"\"A simple attention pool.\n\n        Args:\n            embed_dim (int): embedding dim\n            num_heads (int): number heads\n            dropout (Optional[float], optional): Dropout. Defaults to 0.0.\n            bias (Optional[bool], optional): Whether to use bias or not in MLP layers. Defaults to False.\n            zero_attn (Optional[bool], optional): Passthrough to MHA. Defaults to False.\n            norm (Optional[nn.Module], optional): Passthrough to MHA. Defaults to nn.LayerNorm.\n        \"\"\"\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(\n            embed_dim,\n            bias=bias,\n            dropout=dropout,\n            num_heads=num_heads,\n            add_bias_kv=bias,\n            add_zero_attn=zero_attn,\n        )\n\n        self.norm = norm(embed_dim)\n        self.norm_v = norm(embed_dim)\n        self.norm_k = norm(embed_dim)\n        self.norm2 = norm(embed_dim)\n        self.query_token = nn.Parameter(torch.zeros(1, embed_dim))\n        nn.init.normal_(self.query_token, std=0.02)\n\n        self.dropout_sa = nn.Dropout(dropout)\n\n    def forward(self, x, attn_mask, bs):\n        # query_token = query_token.expand(bs, -1)\n        query_token = self.norm(self.query_token.expand(bs, -1))\n\n        attn_output, attn_output_weights = self.attn(\n            query_token, self.norm_v(x), self.norm_k(x), attn_mask=attn_mask\n        )\n        attn_output = self.dropout_sa(attn_output)\n\n        return self.norm2(attn_output)\n</code></pre>"},{"location":"reference/model/blocks/#model.blocks.AttnPool.__init__","title":"<code>__init__(embed_dim, num_heads, dropout=0.0, bias=False, zero_attn=False, norm=nn.LayerNorm)</code>","text":"<p>A simple attention pool.</p> <p>Args:     embed_dim (int): embedding dim     num_heads (int): number heads     dropout (Optional[float], optional): Dropout. Defaults to 0.0.     bias (Optional[bool], optional): Whether to use bias or not in MLP layers. Defaults to False.     zero_attn (Optional[bool], optional): Passthrough to MHA. Defaults to False.     norm (Optional[nn.Module], optional): Passthrough to MHA. Defaults to nn.LayerNorm.</p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    dropout: Optional[float] = 0.0,\n    bias: Optional[bool] = False,\n    zero_attn: Optional[bool] = False,\n    norm: Optional[nn.Module] = nn.LayerNorm,\n):\n    \"\"\"A simple attention pool.\n\n    Args:\n        embed_dim (int): embedding dim\n        num_heads (int): number heads\n        dropout (Optional[float], optional): Dropout. Defaults to 0.0.\n        bias (Optional[bool], optional): Whether to use bias or not in MLP layers. Defaults to False.\n        zero_attn (Optional[bool], optional): Passthrough to MHA. Defaults to False.\n        norm (Optional[nn.Module], optional): Passthrough to MHA. Defaults to nn.LayerNorm.\n    \"\"\"\n    super().__init__()\n\n    self.attn = nn.MultiheadAttention(\n        embed_dim,\n        bias=bias,\n        dropout=dropout,\n        num_heads=num_heads,\n        add_bias_kv=bias,\n        add_zero_attn=zero_attn,\n    )\n\n    self.norm = norm(embed_dim)\n    self.norm_v = norm(embed_dim)\n    self.norm_k = norm(embed_dim)\n    self.norm2 = norm(embed_dim)\n    self.query_token = nn.Parameter(torch.zeros(1, embed_dim))\n    nn.init.normal_(self.query_token, std=0.02)\n\n    self.dropout_sa = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/model/blocks/#model.blocks.ZINBProj","title":"<code>ZINBProj</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>class ZINBProj(nn.Module):\n\n    def __init__(self, embed_dim: int, n_genes: int, eps: float):\n        \"\"\"A utility block to regress the parameters of a (zero-inflated)\n        negative binomial distribution from an embedding. \n\n        Args:\n            embed_dim (int): embedding dim\n            n_genes (int): number of output genes to regress\n            eps (float): error floor for mu, theta, scale params\n        \"\"\"\n        super().__init__()\n        self.mu = nn.Linear(embed_dim, n_genes)  # mean\n        self.theta = nn.Linear(\n            embed_dim, n_genes\n        )  # inv. dispersion\n        self.scale = nn.Linear(\n            embed_dim, n_genes\n        )  # avg. expr.\n        self.gate_logit = nn.Linear(\n            embed_dim, n_genes\n        )  # zi\n\n        self.eps = eps\n\n    def forward(self, x) -&gt; Dict[str, Float[torch.Tensor, \"n_cells n_genes\"]]:\n        mu = self.mu(x).exp() + self.eps\n        theta = self.theta(x).exp() + self.eps\n        gate = self.gate_logit(x) \n        scale = self.scale(x).exp() + self.eps\n        return dict(mu=mu, theta=theta, zi_logits=gate, scale=scale)\n</code></pre>"},{"location":"reference/model/blocks/#model.blocks.ZINBProj.__init__","title":"<code>__init__(embed_dim, n_genes, eps)</code>","text":"<p>A utility block to regress the parameters of a (zero-inflated) negative binomial distribution from an embedding. </p> <p>Args:     embed_dim (int): embedding dim     n_genes (int): number of output genes to regress     eps (float): error floor for mu, theta, scale params</p> Source code in <code>brainformr/model/blocks.py</code> <pre><code>def __init__(self, embed_dim: int, n_genes: int, eps: float):\n    \"\"\"A utility block to regress the parameters of a (zero-inflated)\n    negative binomial distribution from an embedding. \n\n    Args:\n        embed_dim (int): embedding dim\n        n_genes (int): number of output genes to regress\n        eps (float): error floor for mu, theta, scale params\n    \"\"\"\n    super().__init__()\n    self.mu = nn.Linear(embed_dim, n_genes)  # mean\n    self.theta = nn.Linear(\n        embed_dim, n_genes\n    )  # inv. dispersion\n    self.scale = nn.Linear(\n        embed_dim, n_genes\n    )  # avg. expr.\n    self.gate_logit = nn.Linear(\n        embed_dim, n_genes\n    )  # zi\n\n    self.eps = eps\n</code></pre>"},{"location":"reference/model/ct/","title":"ct","text":""},{"location":"reference/model/ct/#model.ct.CellLocationTransformer","title":"<code>CellLocationTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>brainformr/model/ct.py</code> <pre><code>class CellLocationTransformer(nn.Module):\n    def __init__(\n        self,\n        encoder_embedding_dim: int,\n        encoder_num_heads: int,\n        encoder_depth: int,\n        decoder_embedding_dim: int,\n        decoder_num_heads: int,\n        decoder_depth: int,\n        cell_cardinality: int = 307,\n        device: str = \"cuda\",\n        eps: float = 1e-9,\n        attn_pool: bool = False,\n        attn_pool_heads: int = 8,\n        actvn_last: bool = False,\n        num_output_genes: int = 500,\n    ):\n        \"\"\"An encoder-decoder model with attention pooling prior to decoder. \n\n        Args:\n            encoder_embedding_dim (int): embedding dim \n            encoder_num_heads (int): \n            encoder_depth (int): \n            decoder_embedding_dim (int): \n            decoder_num_heads (int): \n            decoder_depth (int): \n            cell_cardinality (int, optional): Number of cell types. Defaults to 307.\n            device (str, optional): Device for transfering tensors to in `forward`. Defaults to \"cuda\".\n            eps (float, optional): Floor for nonnegative parameters of negative binomial regression layers. Defaults to 1e-9.\n            attn_pool (bool, optional): Whether to use attention pooling or not. Defaults to False.\n            attn_pool_heads (int, optional): Number of attention pool heads. Defaults to 8.\n            num_output_genes (int, optional): Number of genes to regress. Defaults to 500.\n        \"\"\"\n        super().__init__()\n\n        self.encoder_embedding_dim = encoder_embedding_dim\n        self.decoder_embedding_dim = decoder_embedding_dim\n        self.device = device\n\n        self.eps = eps\n        self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n        _feature_dim = encoder_embedding_dim // 2\n\n        self.expression_projection = nn.Sequential(\n            nn.Linear(num_output_genes, _feature_dim),\n            nn.LayerNorm(_feature_dim),\n            nn.GELU(),\n            nn.Linear(_feature_dim, _feature_dim),\n            nn.LayerNorm(_feature_dim),\n            nn.GELU() if actvn_last else nn.Identity(),\n        )\n\n        self.cell_embedding = nn.Embedding(cell_cardinality, _feature_dim)\n        self.decoding_cell_embedding = nn.Embedding(\n            cell_cardinality, encoder_embedding_dim\n        )\n\n        self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n        self.norm = nn.LayerNorm(encoder_embedding_dim)\n        if attn_pool is True:\n            self.attn_pool = AttnPool(\n                encoder_embedding_dim, attn_pool_heads, bias=True, zero_attn=True\n            )\n        else:\n            self.attn_pool = None\n\n        if encoder_depth != 0:\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=encoder_embedding_dim,\n                nhead=encoder_num_heads,\n                activation=\"gelu\",\n                dropout=0.0,\n            )\n\n            encoder_layer.self_attn = nn.MultiheadAttention(\n                encoder_embedding_dim,\n                num_heads=encoder_num_heads,\n                dropout=0.0,\n                add_zero_attn=True,\n            )\n\n            self.encoder = nn.TransformerEncoder(\n                encoder_layer=encoder_layer,\n                num_layers=decoder_depth,\n                enable_nested_tensor=False,\n            )\n        else:\n            self.encoder = None\n\n        decoder_layer = nn.TransformerEncoderLayer(\n            d_model=decoder_embedding_dim,\n            nhead=decoder_num_heads,\n            activation=\"gelu\",\n            dropout=0.0,\n        )\n\n        decoder_layer.self_attn = nn.MultiheadAttention(\n            encoder_embedding_dim,\n            num_heads=decoder_num_heads,\n            dropout=0.0,\n            add_zero_attn=True,\n        )\n\n        self.decoder = nn.TransformerEncoder(\n            encoder_layer=decoder_layer,\n            num_layers=decoder_depth,\n            enable_nested_tensor=False,\n            # num_layers=8,\n        )\n\n        self.mu = nn.Linear(encoder_embedding_dim, num_output_genes)  # mean\n        self.theta = nn.Linear(\n            encoder_embedding_dim, num_output_genes\n        )  # inv. dispersion\n        self.scale = nn.Linear(encoder_embedding_dim, num_output_genes)  # avg. expr.\n        self.gate_logit = nn.Linear(encoder_embedding_dim, num_output_genes)  # zi\n\n        self.pos_encoder = nn.Sequential(\n            # nn.LayerNorm(3),\n            nn.Linear(3, encoder_embedding_dim),\n            nn.LayerNorm(encoder_embedding_dim),\n            nn.GELU(),\n            nn.Linear(encoder_embedding_dim, encoder_embedding_dim),\n            nn.LayerNorm(encoder_embedding_dim),\n            nn.GELU() if fancy_encoding else nn.Identity(),\n        )\n\n        self._init_weights()\n\n    def set_device(self, device: str):\n        self.device = device\n        self.to(device)\n\n    def _init_weights(self):\n        nn.init.normal_(self.cls_token, std=0.02)\n\n        # see: unsure if it has been fixed as of 2023-12-04 https://github.com/pytorch/pytorch/issues/72253\n        # for param_name, param in self.named_parameters():\n        #     if (\n        #         'decoder' in param_name\n        #         and ('linear' in param_name or 'proj' in param_name)\n        #         and 'bias' not in param_name\n        #     ):\n        #         nn.init.kaiming_normal_(param)\n\n        #     if (\n        #         'encoder' in param_name\n        #         and ('linear' in param_name or 'proj' in param_name)\n        #         and 'bias' not in param_name\n        #     ):\n        #         nn.init.kaiming_normal_(param)\n\n    def forward(\n        self,\n        data_dict: Dict[str, torch.Tensor],\n        only_pos_encoder: bool = False,\n    ):\n        bs = data_dict[\"bs\"]\n        cells = data_dict[\"observed_cells\"].to(self.device)\n        center_indices = data_dict[\"center_indices\"].to(self.device, non_blocking=False)\n        attn_mask = data_dict[\"attn_mask\"].to(self.device, non_blocking=True)\n        # repeat_lengths = data_dict[\"hidden_sequence_lengths\"].to(\n        #     self.device, non_blocking=True\n        # )\n        hidden_cells = data_dict[\"hidden_cells\"].to(self.device, non_blocking=False)\n        hidden_expression = data_dict[\"hidden_expression\"].to(\n            self.device, non_blocking=False\n        )\n        expression = data_dict[\"observed_expression\"].to(\n            self.device, non_blocking=False\n        )\n        # pooling_mask = data_dict[\"pooling_mask\"].to(self.device, non_blocking=True)\n        full_mask = data_dict[\"full_mask\"].to(self.device, non_blocking=False)\n        # position_coords = data_dict[\"pos_embed\"].to(self.device, non_blocking=True)\n        if self.attn_pool is not None:\n            post_pool_mask = data_dict[\"post_pooling_mask\"].to(\n                self.device, non_blocking=True\n            )\n            pooling_mask = data_dict[\"pooling_mask\"].to(self.device, non_blocking=True)\n\n        num_tokens_encoder = attn_mask.shape[0]\n        num_obs = cells.shape[0]\n        num_hidden = hidden_cells.shape[0]\n\n        # pos_tokens = self.pos_encoder(center_indices)\n        # pos_tokens = self.pos_encoder(position_coords)\n        pos_tokens = self.cls_token.repeat_interleave(bs, dim=0)\n\n        if len(expression) &gt; 0:\n            cells_embed = self.cell_embedding(cells)\n            # expression_embed = self.expression_projection(expression[:, :500])\n            expression_embed = self.expression_projection(expression)\n            cells_embed = torch.cat((cells_embed, expression_embed), dim=1)\n            cells_embed = torch.cat((cells_embed, pos_tokens), dim=0)\n        else:\n            cells_embed = pos_tokens\n\n        cells_embed = self.norm(cells_embed)\n        # celss_embed = torch.cat((cells_embed, decoding_queries))\n\n        # if only_pos_encoder:\n        #    return dict(pos_tokens=encoded[-bs:, :])\n\n        # pos_tokens = encoded[-bs:]\n        # pos_tokens = self.attn_pool(encoded, self.pooling_token, pooling_mask)\n        # expanded = pos_tokens.repeat_interleave(repeat_lengths, dim=0)\n        # logits = self.cell_clf(expanded)\n\n        if self.attn_pool is None:\n            # print('attn pool is none')\n            decoding_queries = self.decoding_cell_embedding(hidden_cells)\n\n            if self.encoder is not None:\n                #    print('encoder is not none')\n                cells_embed = self.encoder(cells_embed, mask=attn_mask)\n            #                cells_embed = torch.cat()\n\n            all_indices = list(range(full_mask.shape[0]))\n            obs_indices = all_indices[:num_obs]\n            hidden_indices = all_indices[-num_hidden:]\n            # print(full_mask.shape[0], num_hidden, 'alex look here bro')\n\n            # pos_indices = all_indices[num_obs:-num_hidden]\n            pos_indices = all_indices[num_obs : num_obs + bs]\n            indices_to_use = np.array(obs_indices + hidden_indices)\n            early_pos = cells_embed[pos_indices].clone().detach()\n\n            cells_embed = torch.cat((cells_embed, decoding_queries), dim=0)\n            cells_embed = self.decoder(cells_embed, mask=full_mask)\n\n            all_cells_embed = cells_embed[indices_to_use]\n\n        # logits = self.gex_decoder(cells_embed[num_tokens_encoder:])\n\n        # observed_expr = self.gex_decoder(cells_embed[:num_cells])\n        # logits = self.gex_decoder(cells_embed[num_tokens_encoder:])\n        else:\n            cells_embed = self.encoder(cells_embed, mask=attn_mask)\n            attn_pooled = self.attn_pool(cells_embed, pooling_mask, bs)\n            decoding_queries = self.decoding_cell_embedding(hidden_cells)\n            cells_embed = torch.cat((attn_pooled, decoding_queries), dim=0)\n\n            cells_embed = self.decoder(cells_embed, mask=post_pool_mask)\n\n            all_indices = list(range(cells_embed.shape[0]))\n            obs_indices = None\n            # hidden_indices = all_indices[-num_hidden:]\n            hidden_indices = all_indices\n            early_pos = cells_embed[:bs].clone().detach()\n\n            pos_indices = all_indices[:bs]\n            indices_to_use = hidden_indices\n\n            all_cells_embed = cells_embed[-bs:]\n\n        pos_tokens = cells_embed[pos_indices]\n\n        mu = self.mu(all_cells_embed).exp()\n        theta = self.theta(all_cells_embed).exp()\n\n        gate = self.gate_logit(all_cells_embed)\n        scale = self.scale(all_cells_embed).exp()\n\n        all_expression = torch.cat((expression, hidden_expression))\n\n        return dict(\n            zinb_params=dict(mu=mu, scale=scale, theta=theta, zi_logits=gate),\n            mu=mu,\n            theta=theta,\n            gate=gate,\n            scale=scale,\n            early_pos=early_pos,\n            # embeddings=encoded[:bs],\n            decodings=cells_embed,\n            hidden_cells=hidden_cells,\n            pos_tokens=pos_tokens,\n            hidden_expression=hidden_expression,\n            all_expression=all_expression,\n            hidden_indices=np.array(hidden_indices),\n            # all_expression=torch.cat((expression, hidden_expression))\n        )\n</code></pre>"},{"location":"reference/model/ct/#model.ct.CellLocationTransformer.__init__","title":"<code>__init__(encoder_embedding_dim, encoder_num_heads, encoder_depth, decoder_embedding_dim, decoder_num_heads, decoder_depth, cell_cardinality=307, device='cuda', eps=1e-09, attn_pool=False, attn_pool_heads=8, actvn_last=False, num_output_genes=500)</code>","text":"<p>An encoder-decoder model with attention pooling prior to decoder. </p> <p>Args:     encoder_embedding_dim (int): embedding dim      encoder_num_heads (int):      encoder_depth (int):      decoder_embedding_dim (int):      decoder_num_heads (int):      decoder_depth (int):      cell_cardinality (int, optional): Number of cell types. Defaults to 307.     device (str, optional): Device for transfering tensors to in <code>forward</code>. Defaults to \"cuda\".     eps (float, optional): Floor for nonnegative parameters of negative binomial regression layers. Defaults to 1e-9.     attn_pool (bool, optional): Whether to use attention pooling or not. Defaults to False.     attn_pool_heads (int, optional): Number of attention pool heads. Defaults to 8.     num_output_genes (int, optional): Number of genes to regress. Defaults to 500.</p> Source code in <code>brainformr/model/ct.py</code> <pre><code>def __init__(\n    self,\n    encoder_embedding_dim: int,\n    encoder_num_heads: int,\n    encoder_depth: int,\n    decoder_embedding_dim: int,\n    decoder_num_heads: int,\n    decoder_depth: int,\n    cell_cardinality: int = 307,\n    device: str = \"cuda\",\n    eps: float = 1e-9,\n    attn_pool: bool = False,\n    attn_pool_heads: int = 8,\n    actvn_last: bool = False,\n    num_output_genes: int = 500,\n):\n    \"\"\"An encoder-decoder model with attention pooling prior to decoder. \n\n    Args:\n        encoder_embedding_dim (int): embedding dim \n        encoder_num_heads (int): \n        encoder_depth (int): \n        decoder_embedding_dim (int): \n        decoder_num_heads (int): \n        decoder_depth (int): \n        cell_cardinality (int, optional): Number of cell types. Defaults to 307.\n        device (str, optional): Device for transfering tensors to in `forward`. Defaults to \"cuda\".\n        eps (float, optional): Floor for nonnegative parameters of negative binomial regression layers. Defaults to 1e-9.\n        attn_pool (bool, optional): Whether to use attention pooling or not. Defaults to False.\n        attn_pool_heads (int, optional): Number of attention pool heads. Defaults to 8.\n        num_output_genes (int, optional): Number of genes to regress. Defaults to 500.\n    \"\"\"\n    super().__init__()\n\n    self.encoder_embedding_dim = encoder_embedding_dim\n    self.decoder_embedding_dim = decoder_embedding_dim\n    self.device = device\n\n    self.eps = eps\n    self.cls_token = nn.Parameter(torch.zeros(1, encoder_embedding_dim))\n\n    _feature_dim = encoder_embedding_dim // 2\n\n    self.expression_projection = nn.Sequential(\n        nn.Linear(num_output_genes, _feature_dim),\n        nn.LayerNorm(_feature_dim),\n        nn.GELU(),\n        nn.Linear(_feature_dim, _feature_dim),\n        nn.LayerNorm(_feature_dim),\n        nn.GELU() if actvn_last else nn.Identity(),\n    )\n\n    self.cell_embedding = nn.Embedding(cell_cardinality, _feature_dim)\n    self.decoding_cell_embedding = nn.Embedding(\n        cell_cardinality, encoder_embedding_dim\n    )\n\n    self.pooling_token = nn.Parameter(torch.randn(1, encoder_embedding_dim))\n\n    self.norm = nn.LayerNorm(encoder_embedding_dim)\n    if attn_pool is True:\n        self.attn_pool = AttnPool(\n            encoder_embedding_dim, attn_pool_heads, bias=True, zero_attn=True\n        )\n    else:\n        self.attn_pool = None\n\n    if encoder_depth != 0:\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=encoder_embedding_dim,\n            nhead=encoder_num_heads,\n            activation=\"gelu\",\n            dropout=0.0,\n        )\n\n        encoder_layer.self_attn = nn.MultiheadAttention(\n            encoder_embedding_dim,\n            num_heads=encoder_num_heads,\n            dropout=0.0,\n            add_zero_attn=True,\n        )\n\n        self.encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer,\n            num_layers=decoder_depth,\n            enable_nested_tensor=False,\n        )\n    else:\n        self.encoder = None\n\n    decoder_layer = nn.TransformerEncoderLayer(\n        d_model=decoder_embedding_dim,\n        nhead=decoder_num_heads,\n        activation=\"gelu\",\n        dropout=0.0,\n    )\n\n    decoder_layer.self_attn = nn.MultiheadAttention(\n        encoder_embedding_dim,\n        num_heads=decoder_num_heads,\n        dropout=0.0,\n        add_zero_attn=True,\n    )\n\n    self.decoder = nn.TransformerEncoder(\n        encoder_layer=decoder_layer,\n        num_layers=decoder_depth,\n        enable_nested_tensor=False,\n        # num_layers=8,\n    )\n\n    self.mu = nn.Linear(encoder_embedding_dim, num_output_genes)  # mean\n    self.theta = nn.Linear(\n        encoder_embedding_dim, num_output_genes\n    )  # inv. dispersion\n    self.scale = nn.Linear(encoder_embedding_dim, num_output_genes)  # avg. expr.\n    self.gate_logit = nn.Linear(encoder_embedding_dim, num_output_genes)  # zi\n\n    self.pos_encoder = nn.Sequential(\n        # nn.LayerNorm(3),\n        nn.Linear(3, encoder_embedding_dim),\n        nn.LayerNorm(encoder_embedding_dim),\n        nn.GELU(),\n        nn.Linear(encoder_embedding_dim, encoder_embedding_dim),\n        nn.LayerNorm(encoder_embedding_dim),\n        nn.GELU() if fancy_encoding else nn.Identity(),\n    )\n\n    self._init_weights()\n</code></pre>"},{"location":"reference/model/factory/","title":"factory","text":""},{"location":"reference/training/scheduler/","title":"scheduler","text":""},{"location":"reference/training/scheduler/#training.scheduler.get_inverse_sqrt_schedule","title":"<code>get_inverse_sqrt_schedule(optimizer, num_warmup_steps, timescale=None, last_epoch=-1)</code>","text":"<p>Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a warmup period which increases lr linearly from 0 to the initial lr set in the optimizer. Args:     optimizer ([<code>~torch.optim.Optimizer</code>]):         The optimizer for which to schedule the learning rate.     num_warmup_steps (<code>int</code>):         The number of steps for the warmup phase.     timescale (<code>int</code>, optional, defaults to <code>num_warmup_steps</code>):         Time scale.     last_epoch (<code>int</code>, optional, defaults to -1):         The index of the last epoch when resuming training. Return:     <code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>brainformr/training/scheduler.py</code> <pre><code>def get_inverse_sqrt_schedule(\n    optimizer, num_warmup_steps: int, timescale: int = None, last_epoch: int = -1\n):\n    \"\"\"\n    Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a\n    warmup period which increases lr linearly from 0 to the initial lr set in the optimizer.\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        timescale (`int`, *optional*, defaults to `num_warmup_steps`):\n            Time scale.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    # Note: this implementation is adapted from\n    # https://github.com/google-research/big_vision/blob/f071ce68852d56099437004fd70057597a95f6ef/big_vision/utils.py#L930\n\n    if timescale is None:\n        timescale = num_warmup_steps\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        shift = timescale - num_warmup_steps\n        decay = 1.0 / np.sqrt((current_step + shift) / timescale)\n        return decay\n\n    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n</code></pre>"},{"location":"reference/training/scheduler/#training.scheduler.get_inverse_sqrt_schedule_with_plateau","title":"<code>get_inverse_sqrt_schedule_with_plateau(optimizer, num_warmup_steps, timescale=None, last_epoch=-1, plateau_length=0)</code>","text":"<p>Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a warmup period which increases lr linearly from 0 to the initial lr set in the optimizer. Args:     optimizer ([<code>~torch.optim.Optimizer</code>]):         The optimizer for which to schedule the learning rate.     num_warmup_steps (<code>int</code>):         The number of steps for the warmup phase.     timescale (<code>int</code>, optional, defaults to <code>num_warmup_steps</code>):         Time scale.     last_epoch (<code>int</code>, optional, defaults to -1):         The index of the last epoch when resuming training.     plateu_length (<code>int</code>, optional, defaults to 0):         The number of steps for the plateau phase. Return:     <code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>brainformr/training/scheduler.py</code> <pre><code>def get_inverse_sqrt_schedule_with_plateau(\n    optimizer,\n    num_warmup_steps: int,\n    timescale: int = None,\n    last_epoch: int = -1,\n    plateau_length: int = 0,\n):\n    \"\"\"\n    Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a\n    warmup period which increases lr linearly from 0 to the initial lr set in the optimizer.\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        timescale (`int`, *optional*, defaults to `num_warmup_steps`):\n            Time scale.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n        plateu_length (`int`, *optional*, defaults to 0):\n            The number of steps for the plateau phase.\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    # Note: this implementation is adapted from\n    # https://github.com/google-research/big_vision/blob/f071ce68852d56099437004fd70057597a95f6ef/big_vision/utils.py#L930\n\n    if timescale is None:\n        timescale = num_warmup_steps\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        elif current_step &lt; (num_warmup_steps + plateau_length):\n            return 1\n        shift = timescale - num_warmup_steps - plateau_length\n        decay = 1.0 / np.sqrt((current_step + shift) / timescale)\n        return decay\n\n    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n</code></pre>"}]}